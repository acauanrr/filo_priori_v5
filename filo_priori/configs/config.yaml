# Filo-Priori V5 Configuration
# Data processing + SBERT + FT-Transformer pipeline

# Paths
paths:
  train_csv: "../datasets/train.csv"
  test_csv: "../datasets/test_full.csv"
  artifacts_dir: "../artifacts"
  embeddings_dir: "../artifacts/embeddings"
  models_dir: "../artifacts/models"
  reports_dir: "../artifacts/reports"

# Data Processing
data:
  label_column: "TE_Test_Result"
  commit_column: "commit"
  text_semantic_max_length: 2048
  tc_steps_max_chars: 800

  # Label normalization
  fail_labels: ["fail", "failed", "failure"]
  pass_labels: ["pass", "passed", "conditional pass"]
  remove_labels: ["blocked", "pending", "delete", "na", "skip"]

  # Train/val split
  val_split: 0.15
  stratify: true
  group_column: "Build_ID"  # For GroupKFold
  seed: 42

# SBERT Embeddings
sbert:
  model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  batch_size: 32
  device: "cuda"  # or "cpu"
  target_dim: 128  # PCA projection (null for 384D original)
  normalize: false  # Use StandardScaler instead

# FT-Transformer (placeholder - implementar depois se necessário)
model:
  type: "ft_transformer"  # ou "mlp_deep"

  # MLP Deep (alternativa rápida)
  mlp:
    hidden_dims: [512, 256, 128]
    dropout: 0.3
    activation: "relu"
    batch_norm: true

  # FT-Transformer
  ftt:
    n_blocks: 3
    d_token: 192
    attention_n_heads: 4
    attention_dropout: 0.2
    ffn_d_hidden: 384
    ffn_dropout: 0.1
    residual_dropout: 0.0

# Training
training:
  num_epochs: 50
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.01
  scheduler: "onecycle"  # or "cosine"
  warmup_epochs: 3

  # Class imbalance
  use_class_weights: true
  use_focal_loss: false
  focal_gamma: 2.0
  focal_alpha: null  # Auto from class distribution

  # Balanced sampling
  use_balanced_sampler: true
  sampler_positive_fraction: 0.25  # Target 1:3 ratio per batch

  # Early stopping
  patience: 10
  monitor: "val_pr_auc"
  mode: "max"

  # Metrics
  metrics: ["accuracy", "precision", "recall", "f1", "roc_auc", "pr_auc", "apfd"]

  # Threshold optimization
  optimize_threshold: true
  threshold_metric: "f2"  # Emphasize recall
  min_precision: 0.15  # Constraint

# Reproducibility
seed: 42
deterministic: true
