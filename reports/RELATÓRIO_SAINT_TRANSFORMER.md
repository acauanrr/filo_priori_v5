# Relat√≥rio T√©cnico: Modelo SAINT Transformer no Filo-Priori V5

**Autor:** An√°lise do Projeto Filo-Priori V5
**Data:** 2025-10-17
**Vers√£o:** 1.0

---

## üìã Sum√°rio Executivo

Este relat√≥rio documenta em detalhes a arquitetura e funcionamento do **SAINT (Self-Attention and Intersample Attention Transformer)**, o modelo de Deep Learning utilizado no Filo-Priori V5 para classificar casos de teste e predizer falhas.

**Principais Descobertas:**
- SAINT √© um transformer especializado para **dados tabulares** (n√£o texto)
- Combina **self-attention** (dentro de cada amostra) + **intersample attention** (entre amostras)
- Modelo com **~1,86 milh√µes de par√¢metros**
- Arquitetura: 6 camadas, 8 cabe√ßas de aten√ß√£o, embedding interno de 128D
- Treinado com **early stopping** e **cosine learning rate schedule**

---

## üéØ 1. O que √© o SAINT?

### 1.1 Vis√£o Geral

**SAINT** (Self-Attention and Intersample Attention Transformer) √© uma arquitetura transformer criada especificamente para dados tabulares, proposta no paper:

> Somepalli et al., "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training" (NeurIPS 2021)

**Por que SAINT para dados tabulares?**

Transformers tradicionalmente foram criados para sequ√™ncias (texto, √°udio), onde a ordem importa. Em dados tabulares:
- **N√£o h√° ordem natural** entre features (coluna 1 n√£o √© "antes" da coluna 2)
- Features s√£o **heterog√™neas** (dimens√µes diferentes, escalas diferentes)
- Rela√ß√µes entre **amostras** tamb√©m importam (n√£o s√≥ entre features)

SAINT resolve esses problemas com duas inova√ß√µes:
1. **Feature Embedding Layer**: Cada feature √© projetada independentemente
2. **Intersample Attention**: Aten√ß√£o entre diferentes amostras no batch

### 1.2 Compara√ß√£o: SAINT vs MLP vs Transformer Tradicional

| Aspecto | MLP Tradicional | Transformer (BERT-like) | SAINT |
|---------|-----------------|-------------------------|-------|
| **Entrada** | Vetor concatenado | Sequ√™ncia de tokens | Features independentes |
| **Aten√ß√£o** | Nenhuma | Self-attention (sequ√™ncia) | Self + Intersample |
| **Rela√ß√µes** | N√£o-lineares fixas | Entre tokens | Entre features + amostras |
| **Posicionalidade** | N/A | Essencial | N√£o utilizada |
| **Batch** | Independente | Independente | **Dependente (intersample)** |

**Vantagem chave do SAINT:** Aprende a comparar amostras durante treinamento, capturando padr√µes coletivos que MLPs n√£o conseguem.

---

## üèóÔ∏è 2. Arquitetura Completa do SAINT

### 2.1 Vis√£o Geral em Camadas

```
Input: 1028 features (1024 BGE + 4 temporais)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. EMBEDDING LAYER                  ‚îÇ
‚îÇ  1028 features ‚Üí 1028 embeddings 128D‚îÇ
‚îÇ  Total: 1028 √ó (1√ó128) = 131,584 params ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. SAINT BLOCK #1                   ‚îÇ
‚îÇ  - Self-Attention (within sample)    ‚îÇ
‚îÇ  - Intersample Attention (across)    ‚îÇ
‚îÇ  - Feed-Forward MLP                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
     [... 4 blocos intermedi√°rios ...]
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. SAINT BLOCK #6                   ‚îÇ
‚îÇ  (mesma estrutura)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. POOLING LAYER                    ‚îÇ
‚îÇ  Adaptive Average Pooling            ‚îÇ
‚îÇ  [batch, 1028, 128] ‚Üí [batch, 128]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. CLASSIFICATION HEAD              ‚îÇ
‚îÇ  Linear(128 ‚Üí 64) + GELU             ‚îÇ
‚îÇ  Linear(64 ‚Üí 1)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
Output: Logit [batch, 1] ‚Üí Sigmoid ‚Üí Probability [0,1]
```

### 2.2 Contagem de Par√¢metros

**Total: ~1,860,225 par√¢metros**

Distribui√ß√£o:
- **Embedding Layer:** ~131K par√¢metros
- **6 SAINT Blocks:** ~1,700K par√¢metros
- **Classification Head:** ~29K par√¢metros

C√°lculo detalhado por componente est√° na Se√ß√£o 4.

---

## üß© 3. Componentes Detalhados

### 3.1 Embedding Layer

**Objetivo:** Transformar cada feature num√©rica em um vetor denso de 128 dimens√µes.

#### 3.1.1 Arquitetura

```python
class EmbeddingLayer(nn.Module):
    def __init__(self, num_continuous=1028, embedding_dim=128):
        # Cria 1028 Linear layers independentes
        self.continuous_embeddings = nn.ModuleList([
            nn.Linear(1, embedding_dim) for _ in range(num_continuous)
        ])
        self.layer_norm = nn.LayerNorm(embedding_dim)
```

**Por que Linear(1, 128) para cada feature?**
- Cada feature √© tratada **independentemente**
- Aprende transforma√ß√£o espec√≠fica para aquela dimens√£o
- Permite features com diferentes escalas/significados

#### 3.1.2 Forward Pass

```python
# Input: [batch_size, 1028]
x = torch.randn(16, 1028)

# Para cada feature i:
for i in range(1028):
    feat = x[:, i:i+1]           # [16, 1] - extrai feature i
    emb = embed_layer[i](feat)   # [16, 128] - projeta para 128D
    embeddings.append(emb)

# Stack todas as embeddings
embeddings = torch.stack(embeddings, dim=1)  # [16, 1028, 128]
embeddings = layer_norm(embeddings)          # Normaliza
```

**Output:** `[batch_size, 1028, 128]` - 1028 features, cada uma representada por 128 valores

#### 3.1.3 Exemplo Num√©rico

```
Input (1 amostra, primeiras 5 features):
x = [0.42, -0.15, 0.89, 0.31, -0.67, ...]
    ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îò
      ‚îÇ
Feature 1 (0.42) ‚Üí Linear_1 ‚Üí [0.12, -0.34, 0.56, ..., 0.78]  (128 dims)
Feature 2 (-0.15) ‚Üí Linear_2 ‚Üí [-0.23, 0.45, -0.12, ..., 0.34] (128 dims)
Feature 3 (0.89) ‚Üí Linear_3 ‚Üí [0.67, -0.89, 0.23, ..., -0.45] (128 dims)
...

Output:
embeddings = [
  [0.12, -0.34, 0.56, ..., 0.78],  # Feature 1 embedding
  [-0.23, 0.45, -0.12, ..., 0.34], # Feature 2 embedding
  [0.67, -0.89, 0.23, ..., -0.45], # Feature 3 embedding
  ...
]  # Shape: [1028, 128]
```

---

### 3.2 Multi-Head Self-Attention

**Objetivo:** Capturar rela√ß√µes entre diferentes features **dentro da mesma amostra**.

#### 3.2.1 O que √© Self-Attention?

Self-attention permite que cada feature "olhe" para todas as outras features e decida quais s√£o relevantes.

**Exemplo intuitivo:**
- Feature "BGE_dim_50" (sem√¢ntica de "WiFi") pode atender a "BGE_dim_120" (sem√¢ntica de "rede")
- Feature "fail_count" pode atender a "avg_duration" (testes longos que falharam antes)

#### 3.2.2 Mecanismo Matem√°tico

```python
# Input: [batch, seq_len=1028, emb_dim=128]
x = embeddings

# 1. Projeta para Q, K, V
Q = Linear_Q(x)  # [batch, 1028, 128] - Queries
K = Linear_K(x)  # [batch, 1028, 128] - Keys
V = Linear_V(x)  # [batch, 1028, 128] - Values

# 2. Multi-head: divide em 8 cabe√ßas
# [batch, 1028, 128] ‚Üí [batch, 8, 1028, 16]
Q = Q.view(batch, 1028, 8, 16).transpose(1, 2)
K = K.view(batch, 1028, 8, 16).transpose(1, 2)
V = V.view(batch, 1028, 8, 16).transpose(1, 2)

# 3. Calcula scores de aten√ß√£o
# Q √ó K^T: quanto cada feature se relaciona com as outras
scores = (Q @ K.transpose(-2, -1)) / sqrt(16)  # [batch, 8, 1028, 1028]

# 4. Softmax: normaliza para [0,1]
attn_weights = softmax(scores, dim=-1)  # [batch, 8, 1028, 1028]

# 5. Aplica aten√ß√£o aos valores
output = attn_weights @ V  # [batch, 8, 1028, 16]

# 6. Concatena cabe√ßas
output = output.transpose(1, 2).reshape(batch, 1028, 128)
```

#### 3.2.3 Visualiza√ß√£o da Matriz de Aten√ß√£o

```
Attention Weights (1 cabe√ßa, simplificado):
           Feature_1  Feature_2  Feature_3  ...  Feature_1028
Feature_1  [  0.15      0.32      0.05    ...     0.02     ]
Feature_2  [  0.08      0.41      0.23    ...     0.01     ]
Feature_3  [  0.22      0.19      0.35    ...     0.04     ]
...
Feature_1028[ 0.03      0.07      0.12    ...     0.38     ]

Interpreta√ß√£o:
- Feature_1 "atende muito" a Feature_2 (peso 0.32)
- Feature_2 "atende a si mesma" fortemente (0.41 - self-attention)
- Pesos somam 1.0 em cada linha (softmax)
```

#### 3.2.4 Por que Multi-Head (8 cabe√ßas)?

Cada cabe√ßa aprende um "tipo" diferente de rela√ß√£o:
- **Cabe√ßa 1:** Rela√ß√µes sem√¢nticas (BGE features entre si)
- **Cabe√ßa 2:** Rela√ß√µes temporais (hist√≥rico + sem√¢ntica)
- **Cabe√ßa 3:** Correla√ß√µes num√©ricas (dura√ß√£o + falhas)
- ...

Dividir 128D em 8 √ó 16D permite especializa√ß√£o.

---

### 3.3 Intersample Attention (Inova√ß√£o do SAINT)

**Objetivo:** Capturar rela√ß√µes entre diferentes amostras no batch.

#### 3.3.1 Diferen√ßa Fundamental

```
Self-Attention:
- Compara Feature_i com Feature_j na MESMA amostra
- Matriz de aten√ß√£o: [1028 features √ó 1028 features]

Intersample Attention:
- Compara Amostra_i com Amostra_j para a MESMA feature
- Matriz de aten√ß√£o: [batch_size √ó batch_size]
```

#### 3.3.2 Por que √© √∫til?

**Exemplo pr√°tico:**

```
Batch de 16 testes:
- Test_1: "WiFi download" - fail_count=5
- Test_2: "WiFi upload"   - fail_count=0
- Test_3: "WiFi speed"    - fail_count=4
...
- Test_16: "Bluetooth"    - fail_count=0

Intersample Attention permite:
- Test_1 "perceber" que Test_3 √© similar (ambos WiFi + falharam)
- Test_1 "ignorar" Test_16 (dom√≠nio diferente)
- Aprender padr√µes: "testes WiFi tendem a falhar juntos"
```

#### 3.3.3 Implementa√ß√£o

```python
# Input: [batch_size, 1028, 128]
x = embeddings

# 1. TRANSPOSE para [seq_len=1028, batch_size, 128]
x = x.transpose(0, 1)

# 2. Projeta Q, K, V (mesmo conceito)
Q = Linear_Q(x)  # [1028, batch, 128]
K = Linear_K(x)
V = Linear_V(x)

# 3. Multi-head reshape
Q = Q.view(1028, batch, 8, 16).permute(0, 2, 1, 3)  # [1028, 8, batch, 16]

# 4. Aten√ß√£o ENTRE AMOSTRAS (dim=-2 agora √© batch)
scores = (Q @ K.transpose(-2, -1)) / sqrt(16)  # [1028, 8, batch, batch]
attn_weights = softmax(scores, dim=-1)

# 5. Output: [1028, 8, batch, 16] ‚Üí transpose ‚Üí [batch, 1028, 128]
output = attn_weights @ V
output = output.permute(2, 0, 1, 3).reshape(batch, 1028, 128)
```

#### 3.3.4 Visualiza√ß√£o da Matriz Intersample

```
Batch size = 4 (simplificado)
Feature_1 intersample attention:

           Sample_1  Sample_2  Sample_3  Sample_4
Sample_1   [  0.40     0.35     0.15     0.10   ]
Sample_2   [  0.30     0.45     0.20     0.05   ]
Sample_3   [  0.18     0.22     0.50     0.10   ]
Sample_4   [  0.05     0.08     0.12     0.75   ]

Interpreta√ß√£o:
- Sample_1 e Sample_2 se "atendem" mutuamente (0.35, 0.30)
- Sample_4 √© mais isolada (0.75 self-attention, baixa com outras)
- Cada FEATURE tem sua pr√≥pria matriz de aten√ß√£o intersample
```

**Poder do Intersample:** O modelo aprende que "se outros testes similares falharam, eu tamb√©m posso falhar" - generaliza√ß√£o coletiva!

---

### 3.4 SAINT Block Completo

Um bloco SAINT combina:
1. Self-attention (features within sample)
2. Intersample attention (samples across batch)
3. Feed-Forward MLP
4. Residual connections + Layer Normalization

#### 3.4.1 Arquitetura do Bloco

```python
class SAINTBlock(nn.Module):
    def forward(self, x):  # x: [batch, 1028, 128]

        # 1. Self-Attention com residual
        x = x + self.self_attn(self.norm1(x))

        # 2. Intersample Attention com residual (se habilitado)
        if self.use_intersample:
            x = x + self.intersample_attn(self.norm2(x))

        # 3. Feed-Forward MLP com residual
        x = x + self.mlp(self.norm3(x))

        return x  # [batch, 1028, 128]
```

#### 3.4.2 Feed-Forward MLP

```python
# MLP de 2 camadas com GELU
self.mlp = nn.Sequential(
    nn.Linear(128, 512),    # Expande 4x
    nn.GELU(),              # Ativa√ß√£o suave
    nn.Dropout(0.1),
    nn.Linear(512, 128),    # Comprime de volta
    nn.Dropout(0.1)
)
```

**Por que 4x expansion?**
- Padr√£o em transformers (BERT usa 4√ó, GPT tamb√©m)
- Permite representa√ß√µes mais ricas temporariamente
- Comprime de volta para manter dimensionalidade

#### 3.4.3 Residual Connections

```python
# Sem residual (ruim - gradientes somem)
x = self.mlp(self.norm(x))

# Com residual (bom - gradientes fluem)
x = x + self.mlp(self.norm(x))
```

**Benef√≠cios:**
- **Gradient flow:** Gradientes chegam √†s camadas iniciais
- **Identity mapping:** Camada pode aprender "n√£o fazer nada" se necess√°rio
- **Estabilidade:** Evita explos√£o/desaparecimento de gradientes

---

### 3.5 Classification Head

Ap√≥s 6 blocos SAINT, temos `[batch, 1028, 128]`. Precisamos de `[batch, 1]` para classifica√ß√£o.

#### 3.5.1 Pooling

```python
# Input: [batch, 1028, 128]

# Transpose para [batch, 128, 1028]
x = x.transpose(1, 2)

# Adaptive Average Pooling: m√©dia sobre features
x = AdaptiveAvgPool1d(1)(x)  # [batch, 128, 1]

# Remove dimens√£o extra
x = x.squeeze(-1)  # [batch, 128]
```

**Por que average pooling?**
- Agrega informa√ß√£o de todas as 1028 features
- Invariante √† ordem (features n√£o t√™m ordem natural)
- Reduz overfitting (vs. pegar s√≥ 1 feature)

#### 3.5.2 Classifier

```python
self.classifier = nn.Sequential(
    nn.LayerNorm(128),
    nn.Linear(128, 64),      # Reduz dimensionalidade
    nn.GELU(),
    nn.Dropout(0.1),
    nn.Linear(64, 1)         # Sa√≠da bin√°ria
)

# Output: [batch, 1] - logit
logit = classifier(x)

# Converter para probabilidade
prob = torch.sigmoid(logit)  # [0, 1]
```

**Interpreta√ß√£o:**
- `logit > 0` ‚Üí `prob > 0.5` ‚Üí Predi√ß√£o = FAIL
- `logit < 0` ‚Üí `prob < 0.5` ‚Üí Predi√ß√£o = PASS

---

## üî¢ 4. Matem√°tica e Par√¢metros

### 4.1 C√°lculo de Par√¢metros por Componente

#### Embedding Layer
```
Cada feature: Linear(1, 128) = 1 √ó 128 + 128 = 256 par√¢metros
Total: 1028 √ó 256 = 263,168 par√¢metros
LayerNorm(128): 128 √ó 2 = 256 par√¢metros
Subtotal: 263,424 par√¢metros
```

#### Multi-Head Self-Attention (1 m√≥dulo)
```
Q, K, V projections: Linear(128, 128 √ó 3) = 128 √ó 384 + 384 = 49,536
Output projection: Linear(128, 128) = 128 √ó 128 + 128 = 16,512
Subtotal: 66,048 par√¢metros
```

#### Intersample Attention (1 m√≥dulo)
```
Mesma estrutura do self-attention: 66,048 par√¢metros
```

#### Feed-Forward MLP (1 m√≥dulo)
```
Linear(128, 512): 128 √ó 512 + 512 = 66,048
Linear(512, 128): 512 √ó 128 + 128 = 65,664
Subtotal: 131,712 par√¢metros
```

#### SAINT Block Completo (1 bloco)
```
Self-attention: 66,048
Intersample attention: 66,048
MLP: 131,712
3 √ó LayerNorm(128): 3 √ó 256 = 768
Subtotal por bloco: 264,576 par√¢metros
```

#### 6 SAINT Blocks
```
6 √ó 264,576 = 1,587,456 par√¢metros
```

#### Classification Head
```
LayerNorm(128): 256
Linear(128, 64): 128 √ó 64 + 64 = 8,256
Linear(64, 1): 64 √ó 1 + 1 = 65
Subtotal: 8,577 par√¢metros
```

#### **TOTAL GERAL**
```
Embedding:      263,424
6 Blocks:     1,587,456
Classifier:       8,577
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:        1,859,457 par√¢metros ‚âà 1.86M
```

### 4.2 Mem√≥ria e Computa√ß√£o

**Mem√≥ria do modelo (FP32):**
```
1,860,000 params √ó 4 bytes = 7.44 MB
```

**Mem√≥ria durante treinamento (batch_size=16):**
```
Forward pass:
- Embeddings: 16 √ó 1028 √ó 128 √ó 4 bytes = 8.4 MB
- 6 Blocks intermedi√°rios: ~50 MB
- Gradientes: ~7.4 MB
- Optimizer states (AdamW): ~15 MB

Total: ~80-100 MB por batch (estimativa)
```

**FLOPs por forward pass (estimativa):**
```
Embedding: 1028 √ó 128 √ó batch = 131K √ó batch
Self-attention (6 layers): 6 √ó (1028¬≤ √ó 128) √ó batch ‚âà 800M √ó batch
MLP (6 layers): 6 √ó (128 √ó 512 √ó 2) √ó batch ‚âà 0.8M √ó batch

Total: ~800M FLOPs por amostra
Para batch=16: ~12.8 GFLOPs
```

---

## ‚öôÔ∏è 5. Configura√ß√£o e Hiperpar√¢metros

### 5.1 Configura√ß√£o do Modelo (config.yaml)

```yaml
saint:
  num_continuous: 1028          # Features de entrada
  num_categorical: 0            # Sem features categ√≥ricas
  categorical_dims: null
  embedding_dim: 128            # Dimens√£o interna do SAINT
  num_layers: 6                 # N√∫mero de SAINT blocks
  num_heads: 8                  # Cabe√ßas de aten√ß√£o
  mlp_hidden_dim: null          # Default: 4 √ó embedding_dim = 512
  dropout: 0.1                  # Dropout rate
  use_intersample: true         # Ativar intersample attention
```

### 5.2 Configura√ß√£o de Treinamento

```yaml
training:
  num_epochs: 30
  learning_rate: 0.0005         # 5e-4 (conservador)
  weight_decay: 0.01            # L2 regularization
  batch_size: 16
  gradient_clip: 1.0            # Gradient clipping

  # Class imbalance
  pos_weight: 5.0               # BCE loss weight para classe positiva
  use_balanced_sampler: true    # WeightedRandomSampler
  target_positive_fraction: 0.20  # 20% positivos por batch

  # Learning rate schedule
  lr_schedule: "cosine"         # Cosine annealing
  warmup_epochs: 3              # Warmup steps
  min_lr_ratio: 0.01            # Min LR = 0.01 √ó initial LR

  # Early stopping
  patience: 8                   # Epochs sem melhoria
  monitor_metric: "val_auprc"   # M√©trica a monitorar
  validation_split: 0.15        # 15% para valida√ß√£o

  # Regularization
  label_smoothing: 0.05         # Previne overconfidence
```

### 5.3 Hiperpar√¢metros Importantes

| Par√¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| `embedding_dim: 128` | 128D | Balance entre capacidade e efici√™ncia |
| `num_layers: 6` | 6 blocos | Deep enough para padr√µes complexos |
| `num_heads: 8` | 8 cabe√ßas | 128/8 = 16D por cabe√ßa (divis√≠vel) |
| `dropout: 0.1` | 10% | Regulariza√ß√£o moderada |
| `learning_rate: 5e-4` | 0.0005 | Conservador para estabilidade |
| `pos_weight: 5.0` | 5√ó | Compensa desbalanceamento (~5% fails) |
| `label_smoothing: 0.05` | 5% | Previne overfit em labels |
| `patience: 8` | 8 epochs | Permite explorar plat√¥s |
| `monitor_metric: val_auprc` | AUPRC | Melhor que accuracy para dados desbalanceados |

---

## üöÄ 6. Forward Pass Completo - Exemplo Passo a Passo

Vamos seguir **1 amostra** pelo modelo inteiro:

### Input

```python
# 1 teste com 1028 features (1024 BGE + 4 temporais)
x = torch.tensor([[
    # BGE embeddings (1024 dims)
    -0.0234,  0.0156, -0.0421, ...,  0.0423,  # (primeiras/√∫ltimas)
    # Temporal features (4 dims)
    1.0,      # last_run (estava no build anterior)
    0.6931,   # fail_count (log1p(1) = ln(2))
    2.3026,   # avg_duration (log1p(9) = ln(10))
    0.8333    # run_frequency
]])  # Shape: [1, 1028]
```

### Passo 1: Embedding Layer

```python
# Cada feature √© projetada independentemente
embeddings = []
for i in range(1028):
    feat = x[:, i:i+1]              # [1, 1]
    emb = linear_layers[i](feat)    # [1, 128]
    embeddings.append(emb)

embeddings = torch.stack(embeddings, dim=1)  # [1, 1028, 128]
embeddings = layer_norm(embeddings)

# Agora temos 1028 vetores de 128D cada
```

### Passo 2: SAINT Block #1

```python
# Self-Attention
attn_output_1 = multi_head_self_attn(embeddings)
embeddings = embeddings + attn_output_1  # Residual

# Intersample Attention (batch_size=1, n√£o faz muito aqui)
attn_output_2 = intersample_attn(embeddings)
embeddings = embeddings + attn_output_2  # Residual

# Feed-Forward MLP
mlp_output = mlp(embeddings)
embeddings = embeddings + mlp_output  # Residual

# Output: [1, 1028, 128]
```

### Passo 3-7: SAINT Blocks #2-6

```python
# Repete a mesma estrutura 5 vezes
# Cada bloco refina a representa√ß√£o
for block in saint_blocks[1:6]:
    embeddings = block(embeddings)

# Output: [1, 1028, 128]
```

### Passo 8: Pooling

```python
# Transpose: [1, 1028, 128] ‚Üí [1, 128, 1028]
embeddings_t = embeddings.transpose(1, 2)

# Average pooling sobre features: [1, 128, 1028] ‚Üí [1, 128, 1]
pooled = adaptive_avg_pool(embeddings_t)

# Squeeze: [1, 128, 1] ‚Üí [1, 128]
pooled = pooled.squeeze(-1)
```

### Passo 9: Classification Head

```python
# Input: [1, 128]
x = pooled

# Layer 1: Linear(128, 64)
x = linear1(x)  # [1, 64]
x = gelu(x)
x = dropout(x)

# Layer 2: Linear(64, 1)
logit = linear2(x)  # [1, 1]

# Valor exemplo: logit = tensor([0.8234])
```

### Output Final

```python
# Converter logit para probabilidade
prob = torch.sigmoid(logit)  # [1, 1]

# Resultado: prob = tensor([0.6950])
# Interpreta√ß√£o: 69.5% de chance de FALHA
```

---

## üìä 7. Treinamento do SAINT

### 7.1 Loop de Treinamento

```python
def train_saint(model, train_loader, val_loader, config):
    # Setup
    optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)
    criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]))
    scheduler = CosineScheduleWithWarmup(optimizer, warmup_epochs=3)
    early_stopping = EarlyStopping(patience=8, monitor='val_auprc')

    for epoch in range(30):
        # TRAINING
        model.train()
        for batch in train_loader:
            x = batch['continuous']  # [16, 1028]
            y = batch['label']       # [16]

            # Label smoothing
            y_smooth = y * 0.95 + 0.5 * 0.05  # [0‚Üí0.025, 1‚Üí0.975]

            # Forward
            logits = model(x)        # [16, 1]
            loss = criterion(logits, y_smooth.unsqueeze(1))

            # Backward
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        # VALIDATION
        model.eval()
        with torch.no_grad():
            val_metrics = evaluate(model, val_loader)

        # Early stopping
        if early_stopping(val_metrics['auprc']):
            break

        # Save best model
        if val_metrics['auprc'] > best_auprc:
            torch.save(model.state_dict(), 'best_model.pth')
```

### 7.2 Learning Rate Schedule

```python
def cosine_schedule_with_warmup(step, warmup=900, total=9000):
    # Warmup phase (primeiras 3 epochs)
    if step < warmup:
        return step / warmup * 5e-4

    # Cosine decay phase
    progress = (step - warmup) / (total - warmup)
    cosine_decay = 0.5 * (1 + cos(œÄ * progress))
    min_lr = 5e-6  # 0.01 √ó 5e-4
    return min_lr + (5e-4 - min_lr) * cosine_decay

# Exemplo:
# Step 0: LR = 0.0
# Step 450 (meio do warmup): LR = 2.5e-4
# Step 900 (fim do warmup): LR = 5e-4
# Step 4950 (meio do treino): LR ‚âà 2.5e-4
# Step 9000 (fim): LR = 5e-6
```

**Benef√≠cios:**
- **Warmup:** Previne gradientes inst√°veis no in√≠cio
- **Cosine decay:** Permite explorar melhor no final (LR baixa)
- **Min LR ratio:** Nunca para completamente (0.01 √ó original)

### 7.3 Estrat√©gias contra Desbalanceamento

**Problema:** Dataset tem ~95% PASS, 5% FAIL

**Solu√ß√µes aplicadas:**

1. **Pos_weight na loss:**
```python
criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]))
# Fail: peso 5.0
# Pass: peso 1.0
# Loss_fail contribui 5√ó mais que Loss_pass
```

2. **WeightedRandomSampler:**
```python
# Calcula pesos: inverso da frequ√™ncia
weights = [1/count_pass if y==0 else 1/count_fail if y==1 for y in labels]
sampler = WeightedRandomSampler(weights, num_samples=len(dataset))

# Resultado: ~20% FAIL em cada batch (vs 5% no dataset)
```

3. **AUPRC como m√©trica:**
```python
monitor_metric = 'val_auprc'  # Area Under Precision-Recall Curve
# AUPRC √© mais sens√≠vel a classe minorit√°ria que AUC-ROC
```

### 7.4 Early Stopping

```python
class EarlyStopping:
    def __init__(self, patience=8, mode='max'):
        self.patience = patience
        self.best_score = -inf
        self.counter = 0

    def __call__(self, score):
        if score > self.best_score + 1e-4:  # Melhoria significativa
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return True  # Parar treinamento
        return False

# Exemplo de execu√ß√£o:
# Epoch 1: AUPRC=0.015 ‚Üí best=0.015, counter=0
# Epoch 2: AUPRC=0.018 ‚Üí best=0.018, counter=0 (melhorou!)
# Epoch 3: AUPRC=0.019 ‚Üí best=0.019, counter=0
# Epoch 4: AUPRC=0.018 ‚Üí counter=1 (piorou)
# Epoch 5: AUPRC=0.017 ‚Üí counter=2
# ...
# Epoch 12: AUPRC=0.019 ‚Üí counter=8 ‚Üí STOP!
```

---

## üéØ 8. Infer√™ncia e Predi√ß√£o

### 8.1 Carregando Modelo Treinado

```python
# Criar modelo com mesma arquitetura
model = SAINT(
    num_continuous=1028,
    embedding_dim=128,
    num_layers=6,
    num_heads=8,
    dropout=0.1,
    use_intersample=True
)

# Carregar pesos salvos
checkpoint = torch.load('best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()  # Modo de infer√™ncia
```

### 8.2 Predi√ß√£o em Batch

```python
def predict(model, test_loader, device='cuda'):
    model.to(device)
    model.eval()

    all_probs = []

    with torch.no_grad():  # Desativa gradientes
        for batch in test_loader:
            x = batch['continuous'].to(device)  # [batch, 1028]

            # Forward pass
            logits = model(x)           # [batch, 1]
            probs = torch.sigmoid(logits)  # [batch, 1]

            all_probs.append(probs.cpu().numpy())

    # Concatena todos os batches
    all_probs = np.concatenate(all_probs)  # [N_test]

    return all_probs

# Uso:
test_probs = predict(model, test_loader)
# Output: array([0.123, 0.856, 0.034, ..., 0.678])
```

### 8.3 Prioriza√ß√£o de Testes (APFD)

```python
# Ap√≥s obter probabilidades para todos os testes
test_df['prob_fail'] = test_probs

# Ordenar por probabilidade decrescente
test_df_sorted = test_df.sort_values('prob_fail', ascending=False)

# Prioriza√ß√£o:
# 1¬∫: Teste com prob=0.92 (mais prov√°vel de falhar)
# 2¬∫: Teste com prob=0.87
# 3¬∫: Teste com prob=0.85
# ...
# √öltimo: Teste com prob=0.01 (menos prov√°vel)

# Calcular APFD (Average Percentage of Faults Detected)
apfd = calculate_apfd(test_df_sorted['actual_result'])
print(f"APFD: {apfd:.4f}")  # Objetivo: ‚â• 0.70
```

---

## üí° 9. Interpretabilidade do SAINT

### 9.1 Aten√ß√£o como Explica√ß√£o

Podemos extrair pesos de aten√ß√£o para entender decis√µes:

```python
# Durante forward pass, salvar attention weights
model.eval()
attentions = []

def hook_fn(module, input, output):
    attentions.append(output[1])  # Salva pesos de aten√ß√£o

# Registrar hook
model.transformer_blocks[0].self_attn.register_forward_hook(hook_fn)

# Forward pass
with torch.no_grad():
    logits = model(x)

# Analisar aten√ß√£o
attn_weights = attentions[0]  # [batch, num_heads, seq_len, seq_len]

# Para amostra 0, cabe√ßa 0
attn_sample = attn_weights[0, 0]  # [1028, 1028]

# Top-5 features que "Feature_0" atende
top5_indices = attn_sample[0].argsort(descending=True)[:5]
print(f"Feature 0 atende mais a features: {top5_indices}")
# Output: [0, 523, 87, 1015, 342]
# ‚Üí Feature 0 olha para features 523, 87, 1015, 342
```

### 9.2 Feature Importance

```python
# M√©todo 1: Permutation importance
def feature_importance(model, val_loader, baseline_metric):
    importances = []

    for feat_idx in range(1028):
        # Embaralha feature i
        for batch in val_loader:
            batch['continuous'][:, feat_idx] = batch['continuous'][:, feat_idx][torch.randperm(len(batch))]

        # Reavaliar
        new_metric = evaluate(model, val_loader)['auprc']

        # Import√¢ncia = queda no desempenho
        importances.append(baseline_metric - new_metric)

    return np.array(importances)

# Features mais importantes:
# - BGE_dim_50: 0.023 (alta import√¢ncia)
# - fail_count: 0.018
# - avg_duration: 0.012
# - BGE_dim_120: 0.009
```

---

## üîç 10. An√°lise Comparativa

### 10.1 SAINT vs MLP

| Aspecto | MLP (V4) | SAINT (V5) |
|---------|----------|------------|
| **Arquitetura** | 3 camadas densas | 6 transformer blocks |
| **Par√¢metros** | ~250K | ~1.86M |
| **Rela√ß√µes** | Aprende implicitamente | Expl√≠citas (attention) |
| **Batch dependence** | Independente | Intersample attention |
| **Interpretabilidade** | Baixa | M√©dia (attention weights) |
| **Overfitting** | Moderado | Maior risco (mais params) |
| **Tempo treino (GPU)** | ~5 min | ~30-60 min |

### 10.2 Vantagens do SAINT

1. **Modelagem de rela√ß√µes complexas:**
   - MLP: Linear ‚Üí ReLU ‚Üí Linear (n√£o-linearidade fixa)
   - SAINT: Aten√ß√£o adaptativa (aprende quais features combinar)

2. **Generaliza√ß√£o entre amostras:**
   - MLP: Cada amostra √© independente
   - SAINT: Intersample attention captura padr√µes coletivos

3. **Escalabilidade:**
   - MLP: Linear com n√∫mero de features (772‚Üí1028 ok)
   - SAINT: Quadr√°tico, mas paralelo (GPU eficiente)

### 10.3 Desvantagens do SAINT

1. **Custo computacional:**
   - Self-attention: O(1028¬≤ √ó 128) por camada
   - Intersample: O(batch¬≤ √ó 128) por feature

2. **Overfitting em datasets pequenos:**
   - 1.86M params precisam de muitos dados
   - Smoke test (100 builds) √© insuficiente

3. **Complexidade de tuning:**
   - MLP: 3-4 hiperpar√¢metros
   - SAINT: 10+ hiperpar√¢metros

---

## üìö 11. Perguntas Frequentes (FAQ)

### Q1: Por que SAINT e n√£o BERT/GPT?

**R:** BERT/GPT foram criados para sequ√™ncias com ordem temporal (texto). Dados tabulares n√£o t√™m ordem - a coluna 1 n√£o vem "antes" da coluna 2. SAINT:
- Remove positional embeddings
- Trata cada feature independentemente
- Adiciona intersample attention (BERT n√£o tem)

### Q2: Intersample attention funciona com batch_size=1?

**R:** Tecnicamente sim, mas n√£o traz benef√≠cio (compara amostra consigo mesma). Batches maiores (16-32) s√£o ideais para intersample aprender padr√µes entre amostras.

### Q3: Como SAINT lida com features heterog√™neas?

**R:** A Embedding Layer projeta cada feature separadamente via Linear(1, 128). Assim:
- Feature 1 (BGE semantic) aprende sua proje√ß√£o
- Feature 1028 (run_frequency) aprende outra proje√ß√£o
- N√£o h√° "mistura" prematura - s√≥ depois no attention

### Q4: Posso desabilitar intersample attention?

**R:** Sim, mude `use_intersample: false` no config. Resultado:
- SAINT vira um transformer "normal" (s√≥ self-attention)
- Perde capacidade de generaliza√ß√£o coletiva
- ~30% mais r√°pido (1 attention layer a menos por bloco)

### Q5: Como escolher `embedding_dim`?

**R:** Trade-off capacidade vs efici√™ncia:
- **Menor (64D):** Mais r√°pido, pode underfit
- **Padr√£o (128D):** Balance (usado neste projeto)
- **Maior (256D):** Mais expressivo, risco de overfit

Regra: `embedding_dim √ó num_features ‚âà tamanho do dataset / 10`
- 128 √ó 1028 = 131K
- Dataset: ~500K amostras ‚Üí ok!

### Q6: SAINT precisa de normaliza√ß√£o de features?

**R:** Sim! As 1028 features j√° v√™m normalizadas:
- BGE embeddings: StandardScaler aplicado
- Temporais: log1p + escalas [0,1]

Sem normaliza√ß√£o, self-attention dominaria features com maior magnitude.

### Q7: Por que 6 camadas e n√£o 12 (como BERT)?

**R:** BERT processa sequ√™ncias de 512 tokens complexos (palavras). Nossos "tokens" s√£o features num√©ricas mais simples:
- 6 camadas: suficiente para capturar rela√ß√µes
- 12 camadas: risco de overfit (dados tabulares < texto)
- Teste emp√≠rico mostrou 6 ideal

---

## ‚úÖ 12. Checklist de Valida√ß√£o

Para verificar se o SAINT est√° funcionando corretamente:

- [ ] **Arquitetura**
  - [ ] Modelo carrega sem erros
  - [ ] Par√¢metros totais ‚âà 1.86M
  - [ ] Input shape: [batch, 1028]
  - [ ] Output shape: [batch, 1]

- [ ] **Embedding Layer**
  - [ ] 1028 Linear layers criadas
  - [ ] Output: [batch, 1028, 128]
  - [ ] LayerNorm aplicado

- [ ] **Attention**
  - [ ] Self-attention funciona
  - [ ] Intersample attention ativa (se `use_intersample=true`)
  - [ ] Attention weights somam 1.0 (softmax)

- [ ] **Treinamento**
  - [ ] Loss diminui ao longo das √©pocas
  - [ ] Val AUPRC monitorado
  - [ ] Early stopping ativa ap√≥s patience epochs
  - [ ] Melhor modelo salvo em `best_model.pth`

- [ ] **Infer√™ncia**
  - [ ] Modelo carrega de checkpoint
  - [ ] Probabilidades no range [0, 1]
  - [ ] Batch inference funciona

---

## üìà 13. Limita√ß√µes e Melhorias Futuras

### 13.1 Limita√ß√µes Atuais

1. **Tamanho do modelo:**
   - 1.86M params pode overfit em datasets pequenos
   - Smoke test (100 builds) insuficiente

2. **Custo computacional:**
   - O(N¬≤ M) onde N=1028 features, M=batch
   - Lento em CPU (~10√ó mais lento que MLP)

3. **Interpretabilidade limitada:**
   - Attention weights dif√≠ceis de visualizar (1028√ó1028)
   - N√£o √© t√£o expl√≠cito quanto √°rvores de decis√£o

4. **Desbalanceamento de classes:**
   - Apesar de pos_weight, ainda luta com 95/5 split
   - AUPRC ~0.02-0.04 (baixo)

### 13.2 Melhorias Propostas

**Curto prazo:**
1. **Reduzir modelo para smoke test:**
   - `num_layers: 2` ao inv√©s de 6
   - `embedding_dim: 64` ao inv√©s de 128
   - ~300K params (vs 1.86M)

2. **Experimentar outras losses:**
   - Focal Loss (foca em exemplos dif√≠ceis)
   - Dice Loss (otimiza F1 diretamente)

**M√©dio prazo:**
3. **Feature selection:**
   - Reduzir de 1028 para top-500 features mais importantes
   - Acelera 4√ó (500¬≤ vs 1028¬≤)

4. **Ensemble:**
   - Combinar SAINT + MLP + XGBoost
   - Votar por consenso

**Longo prazo:**
5. **Contrastive pre-training:**
   - Paper original do SAINT usa pre-training
   - Aprender representa√ß√µes antes de fine-tuning

6. **Arquitetura adaptativa:**
   - Attention esparsa (Top-K attention)
   - Pruning de camadas menos importantes

---

## üìù 14. Conclus√£o

O **SAINT Transformer** √© uma arquitetura poderosa e inovadora para dados tabulares, que combina:

1. **Self-Attention:** Captura rela√ß√µes entre features dentro de cada amostra
2. **Intersample Attention:** Aprende padr√µes comparando diferentes amostras
3. **Deep Architecture:** 6 camadas para modelar complexidade

**Principais Insights:**

‚úÖ **For√ßas:**
- Modelagem expl√≠cita de rela√ß√µes (vs. MLP impl√≠cito)
- Generaliza√ß√£o coletiva via intersample attention
- Escal√°vel para muitas features (1028)

‚ö†Ô∏è **Desafios:**
- Requer dataset grande (1.86M params)
- Computacionalmente intensivo (O(N¬≤ M))
- Prone a overfit em smoke tests

**Quando usar SAINT:**
- Dataset grande (>50K amostras)
- Muitas features (>100)
- Rela√ß√µes complexas entre amostras (testes similares falham juntos)
- GPU dispon√≠vel

**Quando usar MLP:**
- Dataset pequeno (<10K amostras)
- Poucas features (<100)
- Prioridade: velocidade de treinamento
- CPU apenas

O Filo-Priori V5 escolheu SAINT apostando em seu potencial de generaliza√ß√£o superior. Os resultados finais (APFD) determinar√£o se a complexidade adicional vale a pena.

---

**Documento gerado em:** 2025-10-17
**Vers√£o do Filo-Priori:** V5 (SAINT + BGE)
**Refer√™ncia:** Somepalli et al., "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training", NeurIPS 2021
**Implementa√ß√£o:** `filo_priori/models/saint.py` (456 linhas)
