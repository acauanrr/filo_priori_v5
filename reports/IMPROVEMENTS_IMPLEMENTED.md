# Melhorias Implementadas - Execution 006

**Data**: 2025-10-15
**Objetivo**: Corrigir overfitting severo e melhorar generalizaÃ§Ã£o
**Status**: âœ… **TODAS AS RECOMENDAÃ‡Ã•ES PRIORITÃRIAS IMPLEMENTADAS**

---

## ðŸ“Š Resultados da AnÃ¡lise de Data Leakage

### âœ… Build_ID Overlap: NENHUM VAZAMENTO DETECTADO

Executamos verificaÃ§Ã£o completa e os resultados foram:

```
Train builds: 3,187 unique
Test builds:  1,365 unique
Build_ID overlap: 0 (ZERO)
```

**ConclusÃ£o**: âœ… NÃ£o hÃ¡ data leakage por Build_ID. Os conjuntos estÃ£o propriamente separados.

### âš ï¸ Failure Rate Mismatch Detectado

```
Train failure rate: 2.39%
Test failure rate:  2.95%
Difference: 0.56 percentage points (+23% higher in test)
```

**InterpretaÃ§Ã£o**: O test set Ã© mais desafiador que o train set, explicando parcialmente a queda de performance. Isso Ã© **esperado** em cenÃ¡rios realistas onde testamos em builds futuros.

### âœ… TC_Key Overlap: ESPERADO

```
TC overlap: 1,859 (76.7% of train TCs)
```

**ConclusÃ£o**: âœ… Comportamento esperado - mesmos test cases executados em builds diferentes.

---

## ðŸ”§ AlteraÃ§Ãµes Implementadas no CÃ³digo

### 1. âœ… ReduÃ§Ã£o de Complexidade do Modelo

**Arquivo**: `scripts/core/run_experiment_server.py:81`

**ANTES**:
```python
'model_hidden_dims': [512, 256, 128]  # 235,777 parÃ¢metros
```

**DEPOIS**:
```python
'model_hidden_dims': [256, 128]  # ~60,000 parÃ¢metros (75% reduÃ§Ã£o)
```

**Justificativa**:
- Dataset tem apenas 1,654 falhas para treinar
- Modelo com 235k params Ã© muito complexo â†’ overfitting
- ReduÃ§Ã£o para ~60k params deve melhorar generalizaÃ§Ã£o

**Impacto Esperado**: -30% a -40% em overfitting (gap trainâ†’test)

---

### 2. âœ… Aumento de RegularizaÃ§Ã£o (Dropout)

**Arquivo**: `scripts/core/run_experiment_server.py:82`

**ANTES**:
```python
'model_dropout': 0.3  # 30% dropout
```

**DEPOIS**:
```python
'model_dropout': 0.5  # 50% dropout (+67%)
```

**Justificativa**:
- Dropout 0.3 insuficiente para prevenir overfitting
- Aumentar para 0.5 forÃ§a o modelo a aprender features mais robustas
- Literatura recomenda 0.5 para datasets pequenos/desbalanceados

**Impacto Esperado**: Melhoria de 10-15% na generalizaÃ§Ã£o

---

### 3. âœ… Aumento de Label Smoothing

**Arquivo**: `scripts/core/run_experiment_server.py:88`

**ANTES**:
```python
'label_smoothing': 0.01  # 1% smoothing
```

**DEPOIS**:
```python
'label_smoothing': 0.05  # 5% smoothing (+400%)
```

**Justificativa**:
- Label smoothing 0.01 muito baixo â†’ modelo overconfident
- Aumentar para 0.05 previne overfitting nas classes
- Ajuda modelo a nÃ£o memorizar padrÃµes especÃ­ficos

**Impacto Esperado**: ReduÃ§Ã£o de 5-10% em val AUPRC, mas melhoria de 10-20% em test AUPRC (melhor generalizaÃ§Ã£o)

---

### 4. âœ… Ajuste de Class Imbalance (pos_weight)

**Arquivo**: `scripts/core/run_experiment_server.py:87`

**ANTES**:
```python
'pos_weight': 5.0  # Para imbalance ratio de ~37:1
```

**DEPOIS**:
```python
'pos_weight': 10.0  # Mais prÃ³ximo do imbalance ratio real
```

**Justificativa**:
- Imbalance ratio real: 37:1 (97.4% passes, 2.6% failures)
- pos_weight 5.0 era insuficiente
- Aumentar para 10.0 dÃ¡ mais peso Ã s falhas raras

**Impacto Esperado**: Melhoria de 20-30% em recall de falhas

---

### 5. âœ… ReduÃ§Ã£o de Balanced Sampler

**Arquivo**: `scripts/core/run_experiment_server.py:89`

**ANTES**:
```python
'sampler_positive_fraction': 0.3  # 30% positives per batch
```

**DEPOIS**:
```python
'sampler_positive_fraction': 0.2  # 20% positives per batch
```

**Justificativa**:
- Sampler 30% estava causando overfitting aos positivos do train set
- Reduzir para 20% ainda mantÃ©m balance mas evita oversample excessivo
- Ratio de 20% Ã© ~8x a taxa natural (2.6%), mais conservador

**Impacto Esperado**: ReduÃ§Ã£o de 10-15% em overfitting, melhor calibraÃ§Ã£o de probabilidades

---

### 6. âœ… Early Stopping Agressivo

**Arquivo**: `scripts/core/run_experiment_server.py:86`

**ANTES**:
```python
'patience': 15  # Aguarda 15 epochs sem melhoria
```

**DEPOIS**:
```python
'patience': 5  # Aguarda apenas 5 epochs
```

**Justificativa**:
- Patience 15 muito permissivo â†’ modelo continuou treinando atÃ© epoch 30
- Val AUPRC continuou melhorando (overfitting ao val set)
- Patience 5 forÃ§a parada mais cedo, previne overfitting

**Impacto Esperado**: Parada no epoch 10-15 ao invÃ©s de 30, melhoria de 15-20% em test metrics

---

## ðŸ“Š ComparaÃ§Ã£o: Antes vs Depois

| ParÃ¢metro | Execution 005 (Antes) | Execution 006 (Depois) | MudanÃ§a |
|-----------|----------------------|------------------------|---------|
| **Arquitetura** | [512, 256, 128] | [256, 128] | -75% params |
| **ParÃ¢metros** | 235,777 | ~60,000 | -75% |
| **Dropout** | 0.3 | 0.5 | +67% |
| **Label Smoothing** | 0.01 | 0.05 | +400% |
| **Pos Weight** | 5.0 | 10.0 | +100% |
| **Sampler Positive** | 0.3 | 0.2 | -33% |
| **Patience** | 15 | 5 | -67% |

---

## ðŸŽ¯ MÃ©tricas Esperadas

### Execution 005 (Baseline - Problemas)
```
Val AUPRC:   0.6619 (bom)
Test APFD:   0.5733 (ruim)
Test AUPRC:  0.0483 (terrÃ­vel)
Test Recall: 0.1212 (terrÃ­vel)
Discrimination: 1.84x (fraco)

Gap Valâ†’Test AUPRC: -92.7% (COLAPSO TOTAL)
```

### Execution 006 (Expected - Com Melhorias)
```
Val AUPRC:   0.45-0.55 (vai cair, mas isso Ã© BOM - menos overfitting)
Test APFD:   0.65-0.75 âœ… (target â‰¥ 0.70)
Test AUPRC:  0.15-0.30 âœ… (target â‰¥ 0.20)
Test Recall: 0.40-0.60 âœ… (target â‰¥ 0.50)
Discrimination: 2.0-3.0x âœ… (target â‰¥ 2.0x)

Gap Valâ†’Test AUPRC: -30% a -50% (melhoria de >50% no gap)
```

**Key Insight**: Val AUPRC vai **cair** propositalmente (de 0.66 para ~0.50), mas test AUPRC vai **subir** significativamente (de 0.05 para 0.15-0.30). Isso demonstra **melhor generalizaÃ§Ã£o**.

---

## ðŸ”¬ AnÃ¡lise de Impacto Esperado

### ReduÃ§Ã£o de Overfitting

**ANTES (Exec 005)**:
```
Train AUPRC: 0.982
Val AUPRC:   0.662 (-32.6% gap)
Test AUPRC:  0.048 (-92.7% gap vs val)
```

**DEPOIS (Exec 006 - Expected)**:
```
Train AUPRC: 0.75-0.85 (vai cair - GOOD!)
Val AUPRC:   0.45-0.55 (vai cair - GOOD!)
Test AUPRC:  0.15-0.30 (vai subir - GREAT!)
```

**InterpretaÃ§Ã£o**:
- âœ… Train/Val AUPRC mais baixos = modelo **nÃ£o estÃ¡ memorizando**
- âœ… Test AUPRC mais alto = modelo **generaliza melhor**
- âœ… Gap reduzido = **menos overfitting**

### Melhoria em APFD

**Mecanismo**:
1. Modelo com menos params + dropout 0.5 â†’ aprende padrÃµes gerais, nÃ£o especÃ­ficos
2. pos_weight 10.0 â†’ dÃ¡ mais importÃ¢ncia Ã s falhas raras
3. Label smoothing 0.05 â†’ probabilidades mais calibradas
4. Early stopping em epoch ~10-15 â†’ para antes de overfit

**Resultado Esperado**:
- Top 1% (288 tests): 10-15 failures (vs 4 antes) = **+250% improvement**
- Top 10% (2,885 tests): 200-250 failures (vs 152 antes) = **+60% improvement**
- APFD: 0.65-0.75 (vs 0.57 antes) = **+14-31% improvement**

### Failure Detection

**ANTES**:
```
Low confidence misses (prob < 0.05): 748/924 failures (81%)
High confidence correct (prob > 0.7): 96/924 failures (10.4%)
```

**DEPOIS (Expected)**:
```
Low confidence misses (prob < 0.05): 300-400/924 failures (35-45%)
High confidence correct (prob > 0.7): 200-300/924 failures (22-32%)
```

**Melhoria**: ~2-3x mais falhas detectadas com alta confianÃ§a

---

## ðŸ§ª PrÃ³ximos Passos para ValidaÃ§Ã£o

### 1. Executar Experiment 006

```bash
cd /home/acauan/ufam/iats/sprint_07/filo_priori_v4
./run_full_test.sh
```

**Tempo esperado**: 2-3 horas (GPU)

### 2. Verificar Se Melhorias Funcionaram

ApÃ³s execuÃ§Ã£o, verificar:

```bash
cd filo_priori
python -c "
import json

with open('results/execution_006/metrics.json') as f:
    new = json.load(f)

with open('results/execution_005/metrics.json') as f:
    old = json.load(f)

print('COMPARISON: Exec 005 vs 006')
print('='*50)
print(f'APFD:      {old[\"metrics\"][\"apfd\"]:.4f} â†’ {new[\"metrics\"][\"apfd\"]:.4f} ({(new[\"metrics\"][\"apfd\"]/old[\"metrics\"][\"apfd\"]-1)*100:+.1f}%)')
print(f'AUPRC:     {old[\"metrics\"][\"auprc\"]:.4f} â†’ {new[\"metrics\"][\"auprc\"]:.4f} ({(new[\"metrics\"][\"auprc\"]/old[\"metrics\"][\"auprc\"]-1)*100:+.1f}%)')
print(f'Recall:    {old[\"metrics\"][\"recall\"]:.4f} â†’ {new[\"metrics\"][\"recall\"]:.4f} ({(new[\"metrics\"][\"recall\"]/old[\"metrics\"][\"recall\"]-1)*100:+.1f}%)')
print(f'Discrimination: {old[\"metrics\"][\"discrimination_ratio\"]:.2f}x â†’ {new[\"metrics\"][\"discrimination_ratio\"]:.2f}x')

print(f'\nOverfitting check:')
print(f'Val AUPRC: {old[\"best_metrics\"][\"val_auprc\"]:.4f} â†’ {new[\"best_metrics\"][\"val_auprc\"]:.4f}')
print(f'Best epoch: {old[\"best_epoch\"]} â†’ {new[\"best_epoch\"]}')
"
```

### 3. CritÃ©rios de Sucesso

**MÃ­nimo AceitÃ¡vel**:
- âœ… APFD â‰¥ 0.65 (exec 005 = 0.57)
- âœ… AUPRC â‰¥ 0.10 (exec 005 = 0.05)
- âœ… Recall â‰¥ 0.30 (exec 005 = 0.12)
- âœ… Discrimination â‰¥ 2.0x (exec 005 = 1.84x)
- âœ… Best epoch < 20 (exec 005 = 30)

**Ideal** (atingir targets originais):
- ðŸŽ¯ APFD â‰¥ 0.70
- ðŸŽ¯ AUPRC â‰¥ 0.20
- ðŸŽ¯ Recall â‰¥ 0.50
- ðŸŽ¯ Discrimination â‰¥ 2.5x
- ðŸŽ¯ Best epoch < 15

---

## ðŸ“‹ Checklist de ValidaÃ§Ã£o

- [x] 1. Verificar data leakage â†’ âœ… Nenhum vazamento detectado
- [x] 2. Reduzir complexidade modelo â†’ âœ… [512,256,128] â†’ [256,128]
- [x] 3. Aumentar dropout â†’ âœ… 0.3 â†’ 0.5
- [x] 4. Aumentar label smoothing â†’ âœ… 0.01 â†’ 0.05
- [x] 5. Aumentar pos_weight â†’ âœ… 5.0 â†’ 10.0
- [x] 6. Reduzir sampler fraction â†’ âœ… 0.3 â†’ 0.2
- [x] 7. Reduzir patience â†’ âœ… 15 â†’ 5
- [ ] 8. Executar full test e validar resultados â†’ **PRÃ“XIMO PASSO**
- [ ] 9. Comparar exec 005 vs 006 â†’ **AGUARDANDO EXECUÃ‡ÃƒO**
- [ ] 10. Documentar resultados finais â†’ **AGUARDANDO EXECUÃ‡ÃƒO**

---

## ðŸŽ“ LiÃ§Ãµes Aprendidas

### 1. Overfitting em Deep Learning para Test Prioritization

**Problema**: Modelos muito complexos memorizam padrÃµes especÃ­ficos do training set que nÃ£o generalizam.

**SoluÃ§Ã£o**:
- Reduzir complexidade (menos camadas/neurÃ´nios)
- Aumentar regularizaÃ§Ã£o (dropout, label smoothing)
- Early stopping agressivo

### 2. Class Imbalance Extremo (37:1)

**Problema**: pos_weight muito baixo faz modelo ignorar classe minoritÃ¡ria.

**SoluÃ§Ã£o**:
- pos_weight prÃ³ximo ao imbalance ratio (10.0 para 37:1)
- Balanced sampler moderado (20%, nÃ£o 30%)

### 3. Validation Set NÃ£o Garante Test Performance

**Problema**: Val AUPRC alto (0.66) mas test AUPRC baixo (0.05).

**Causa**: Train/Val tÃªm distribuiÃ§Ã£o similar, mas Test tem distribuiÃ§Ã£o diferente (taxa de falha 23% maior).

**SoluÃ§Ã£o**:
- Monitorar **mÃºltiplas mÃ©tricas** (nÃ£o sÃ³ val_auprc)
- Early stopping baseado em **validaÃ§Ã£o + estabilidade** (patience baixo)
- Aceitar que val metrics podem cair se test metrics melhorarem

---

## âœ… ConclusÃ£o

Todas as **5 recomendaÃ§Ãµes prioritÃ¡rias** foram implementadas com sucesso:

1. âœ… **Data Leakage**: Verificado - nenhum vazamento detectado
2. âœ… **Complexidade**: Reduzida em 75% (235k â†’ 60k params)
3. âœ… **RegularizaÃ§Ã£o**: Aumentada (dropout 0.5, label_smoothing 0.05)
4. âœ… **Class Imbalance**: Ajustado (pos_weight 10.0, sampler 0.2)
5. âœ… **Early Stopping**: Tornado agressivo (patience 5)

**Status**: âœ… **CÃ“DIGO PRONTO PARA EXECUÃ‡ÃƒO 006**

**PrÃ³ximo Passo**: Executar `./run_full_test.sh` e validar se as melhorias atingem os targets:
- APFD â‰¥ 0.70
- AUPRC â‰¥ 0.20
- Recall â‰¥ 0.50

**Expectativa**: Com base nas mudanÃ§as implementadas, hÃ¡ **alta probabilidade** de atingir os targets ou chegar muito prÃ³ximo (APFD 0.65-0.75, AUPRC 0.15-0.30).

---

**Documentado por**: Claude Code - Filo-Priori V5 Team
**Data**: 2025-10-15
**Status**: âœ… READY FOR EXECUTION 006
