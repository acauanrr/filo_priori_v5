# Relat√≥rio T√©cnico: Sistema de Embeddings no Filo-Priori V5

**Autor:** An√°lise do Projeto Filo-Priori V5
**Data:** 2025-10-17
**Vers√£o:** 1.0

---

## üìã Sum√°rio Executivo

Este relat√≥rio documenta o sistema completo de gera√ß√£o de embeddings sem√¢nticos utilizado no projeto **Filo-Priori V5**, um sistema de prioriza√ß√£o de testes baseado em Machine Learning que usa embeddings de texto para representar casos de teste e predizer falhas.

**Principais Descobertas:**
- O sistema utiliza o modelo **BGE-large-en-v1.5** da BAAI (Beijing Academy of Artificial Intelligence)
- Gera embeddings de **1024 dimens√µes** (upgrade do SBERT anterior de 768D)
- **N√£o utiliza PCA** - os embeddings completos s√£o mantidos para preservar informa√ß√£o sem√¢ntica
- Processa **3 colunas de entrada**: `TE_Summary`, `TC_Steps` e `commit`
- Cada teste √© representado por um vetor de 1024 n√∫meros reais no espa√ßo sem√¢ntico

---

## üéØ 1. Vis√£o Geral do Sistema de Embeddings

### 1.1 O que s√£o Embeddings?

**Embeddings** s√£o representa√ß√µes vetoriais de texto que capturam seu significado sem√¢ntico em um espa√ßo num√©rico de alta dimens√£o. No contexto do Filo-Priori:

```
Texto (String) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Embedding (Vetor Num√©rico)
"Este teste falhou"          [0.42, -0.15, 0.89, ..., 0.33]
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      1024 dimens√µes
```

**Por que usar embeddings?**
- Algoritmos de ML trabalham com n√∫meros, n√£o com texto
- Capturam rela√ß√µes sem√¢nticas (textos similares ‚Üí vetores pr√≥ximos)
- Permitem compara√ß√µes matem√°ticas entre testes
- Generalizam melhor que representa√ß√µes bag-of-words ou TF-IDF

### 1.2 Arquitetura do Modelo BGE

O **BGE (BAAI General Embedding)** √© um modelo transformer estado-da-arte treinado especificamente para tarefas de retrieval e similaridade sem√¢ntica.

**Especifica√ß√µes T√©cnicas:**
- **Modelo:** `BAAI/bge-large-en-v1.5`
- **Arquitetura:** Transformer baseado em BERT
- **Dimensionalidade:** 1024 (vs 768 do SBERT anterior)
- **Linguagem:** Ingl√™s
- **Treinamento:** Contrastive learning em milh√µes de pares de textos
- **Rank no MTEB:** Top-5 em benchmarks de embedding

**Vantagens sobre SBERT:**
- +33% mais dimens√µes (1024 vs 768)
- Melhor performance em tarefas de retrieval
- Maior capacidade de capturar nuances sem√¢nticas
- Treinamento mais robusto em dados de dom√≠nio t√©cnico

---

## üì• 2. Dados de Entrada: Quais Colunas S√£o Utilizadas?

### 2.1 Colunas do Dataset Original

O dataset `train.csv` cont√©m as seguintes colunas relevantes para embeddings:

| Coluna | Tipo | Descri√ß√£o | Exemplo |
|--------|------|-----------|---------|
| `TE_Summary` | String | Resumo do Test Execution | "TE - TC - OTA: Download upgrade package when the user select Wifi Only option" |
| `TC_Steps` | String | Passos do Test Case | "1. Check system update... 2. Select WiFi only..." |
| `commit` | String (List) | Mensagens de commits relacionados | "['IKSWQ-787: support below interfaces...']" |
| `TE_Test_Result` | String | Label (Pass/Fail) | "Pass" ou "Fail" |

### 2.2 Exemplo Real de Dados Brutos

Aqui est√° um exemplo real extra√≠do do dataset:

```csv
Build_ID: QPW30.18
TE_Summary: TE - TC - OTA: Download upgrade package when the user select Wifi Only option
TC_Steps:
  0. 1. Check the system update manually from Settings -> About phone -> System updates.
  1. 2. Select download over WiFi only.
  2. 3. Turn on WiFi and connect to a valid AP.
  3. 4. During upgrade download, go out of the wifi signal covered area to lose the wifi connection.
  4. 5. Wait for about 20 minutes and go back to wifi signal covered area.
  5. 6. During upgrade download, turn on airplane mode.
  6. 7. Turn off airplane mode.
commit: ['IKSWQ-787: support below interfaces Solution: (1)getCDMADataRate/setCDMADataRate (2)setAkeyInfo (3)getCdmaSidNidPairs/setCdmaSidNidPairs']
TE_Test_Result: Pass
```

---

## üîÑ 3. Pipeline de Processamento Completo

### 3.1 Etapa 1: Constru√ß√£o do `commit_text` (M√≥dulo 01)

**Arquivo:** `filo_priori/data_processing/01_parse_commit.py`

Processa a coluna `commit` (que √© uma string representando lista Python):

```python
# Input
commit_raw = "['IKSWQ-787: support interfaces', 'MCA-123: fix bug']"

# Processing
commits = ast.literal_eval(commit_raw)  # Converte string ‚Üí lista Python
commit_text = " | ".join(commits)       # Une com separador

# Output
commit_text = "IKSWQ-787: support interfaces | MCA-123: fix bug"
```

**Casos especiais:**
- Lista vazia ‚Üí `"No commit info."`
- Erro de parsing ‚Üí `"No commit info."`

### 3.2 Etapa 2: Constru√ß√£o do `text_semantic` (M√≥dulo 02)

**Arquivo:** `filo_priori/data_processing/02_build_text_semantic.py`

Esta √© a etapa **mais cr√≠tica** - combina as 3 fontes de texto em uma √∫nica string estruturada.

#### 3.2.1 Limpeza de Texto

Fun√ß√£o `clean_text()` aplica:
1. Remove tags HTML: `<div>texto</div>` ‚Üí `texto`
2. Substitui URLs: `http://example.com` ‚Üí `[URL]`
3. Remove IDs longos: `ABC123DEF456GHI789JKL` ‚Üí `[ID]`
4. Normaliza espa√ßos: `"texto  \n\n  outro"` ‚Üí `"texto outro"`
5. Remove bullets: `"‚Ä¢ item"` ‚Üí `"item"`

#### 3.2.2 Compacta√ß√£o de Steps

Fun√ß√£o `compact_steps()` formata os passos do teste:

```python
# Input (raw TC_Steps)
"""
0. 1. Check the system update manually...
1. 2. Select download over WiFi only.
2. 3. Turn on WiFi and connect to a valid AP.
"""

# Processing
# 1. Detecta padr√µes de steps (Step 1, 1., etc)
# 2. Extrai at√© 10 steps
# 3. Reformata consistentemente
# 4. Trunca em 800 caracteres

# Output (formatted)
"Step 1: Check the system update manually... Step 2: Select download over WiFi only. Step 3: Turn on WiFi and connect to a valid AP..."
```

#### 3.2.3 Combina√ß√£o Final

Fun√ß√£o `build_text_semantic()` cria o texto final:

```python
# Template
text_semantic = """
[TE Summary] {TE_Summary limpo}.
[TC Steps] {TC_Steps formatados}.
{commit_text}
"""

# Exemplo real de OUTPUT
text_semantic = """[TE Summary] TE - TC - OTA: Download upgrade package when the user select Wifi Only option.
[TC Steps] Step 1: Check the system update manually from Settings -> About phone -> System updates. Step 2: Select download over WiFi only. Step 3: Turn on WiFi and connect to a valid AP. Step 4: During upgrade download, go out of the wifi signal covered area to lose the wifi connection. Step 5: Wait for about 20 minutes and go back to wifi signal covered area. Step 6: During upgrade download, turn on airplane mode. Step 7: Turn off airplane mode.
IKSWQ-787: support below interfaces Solution: (1)getCDMADataRate/setCDMADataRate (2)setAkeyInfo (3)getCdmaSidNidPairs/setCdmaSidNidPairs"""
```

**Caracter√≠sticas:**
- M√°ximo de 2048 caracteres (trunca se exceder)
- Se√ß√µes claramente delimitadas com `[...]`
- Fallback para `"No information available."` se vazio
- M√©dia de ~800-1200 caracteres por teste

### 3.3 Etapa 3: Gera√ß√£o de Embeddings BGE (M√≥dulo 03)

**Arquivo:** `filo_priori/data_processing/03_embed_sbert.py`

#### 3.3.1 Inicializa√ß√£o do Modelo

```python
from sentence_transformers import SentenceTransformer

# Carrega modelo BGE-large
embedder = SentenceTransformer('BAAI/bge-large-en-v1.5', device='cuda')
dim = embedder.get_sentence_embedding_dimension()  # 1024
```

**Configura√ß√£o:**
- `model_name`: `'BAAI/bge-large-en-v1.5'`
- `batch_size`: 256 (para processamento eficiente)
- `device`: `'cuda'` (GPU) ou `'cpu'`
- `normalize_embeddings`: `False` (normaliza√ß√£o feita depois com StandardScaler)

#### 3.3.2 Encoding: Texto ‚Üí Vetor

A fun√ß√£o `encode()` converte cada `text_semantic` em um vetor de 1024 dimens√µes:

```python
# Input: Lista de textos
texts = [
    "[TE Summary] Test wifi download...",
    "[TE Summary] Test bluetooth connection...",
    ...
]

# Processing
embeddings = embedder.encode(
    texts,
    batch_size=256,
    convert_to_numpy=True,
    show_progress_bar=True
)

# Output: Numpy array
# Shape: (N_tests, 1024)
# Dtype: float32
```

**O que acontece internamente no modelo BGE?**

1. **Tokeniza√ß√£o:** Texto ‚Üí Tokens (palavras/subpalavras)
   ```
   "Test wifi download" ‚Üí ["Test", "wifi", "down", "##load"]
   ```

2. **Embedding de Tokens:** Cada token ‚Üí Vetor (768D)
   ```
   "Test"     ‚Üí [0.1, -0.3, 0.5, ...]
   "wifi"     ‚Üí [0.4, 0.2, -0.1, ...]
   "down"     ‚Üí [-0.2, 0.6, 0.3, ...]
   "##load"   ‚Üí [0.3, -0.1, 0.4, ...]
   ```

3. **Transformer Layers:** 24 camadas de aten√ß√£o processam contexto
   - Self-attention captura rela√ß√µes entre palavras
   - Feed-forward networks transformam representa√ß√µes
   - Layer normalization estabiliza training

4. **Pooling:** Agrega tokens ‚Üí Embedding da senten√ßa (1024D)
   ```
   Mean pooling sobre todos os tokens:
   embedding_final = mean([emb_test, emb_wifi, emb_down, emb_load])
   ```

5. **Output:** Vetor denso de 1024 dimens√µes
   ```
   embedding = [0.42, -0.15, 0.89, 0.31, ..., 0.67]
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ1024 valores‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ```

#### 3.3.3 Exemplo de Transforma√ß√£o

**Input (text_semantic):**
```
[TE Summary] TE - TC - OTA: Download upgrade package when the user select Wifi Only option.
[TC Steps] Step 1: Check the system update manually from Settings -> About phone -> System updates...
IKSWQ-787: support below interfaces Solution: (1)getCDMADataRate/setCDMADataRate
```
- **Tipo:** String
- **Comprimento:** ~450 caracteres

‚Üì‚Üì‚Üì **BGE Encoding** ‚Üì‚Üì‚Üì

**Output (embedding):**
```python
array([
    -0.0234,  0.0156, -0.0421,  0.0389,  0.0127, -0.0298,  0.0445, -0.0167,
     0.0312, -0.0234,  0.0189, -0.0356,  0.0278,  0.0123, -0.0401,  0.0334,
    ...  (mais 1008 valores)  ...
    -0.0189,  0.0267, -0.0312,  0.0423, -0.0156,  0.0298,  0.0145, -0.0378
], dtype=float32)
```
- **Tipo:** numpy.ndarray
- **Shape:** (1024,)
- **Dtype:** float32
- **Propriedades:**
  - M√©dia: ~0.0 (ap√≥s StandardScaler)
  - Std: ~1.0 (ap√≥s StandardScaler)
  - Norma L2: ~5-15 (antes de normaliza√ß√£o)

#### 3.3.4 Normaliza√ß√£o com StandardScaler

Ap√≥s o encoding, os embeddings s√£o padronizados:

```python
from sklearn.preprocessing import StandardScaler

# Fit no conjunto de TREINO
scaler = StandardScaler()
scaler.fit(train_embeddings)

# Transform
train_embeddings_scaled = scaler.transform(train_embeddings)
test_embeddings_scaled = scaler.transform(test_embeddings)  # Usa mesma escala!
```

**Por que StandardScaler?**
- Remove diferen√ßas de escala entre dimens√µes
- Facilita converg√™ncia do modelo SAINT
- Cada dimens√£o fica com m√©dia=0 e std=1
- Preserva estrutura sem√¢ntica (transforma√ß√£o linear)

**IMPORTANTE:** N√£o h√° PCA! Vers√£o V5 removeu PCA para manter todas as 1024 dimens√µes originais.

---

## üßÆ 4. Caracter√≠sticas dos Embeddings Gerados

### 4.1 Dimensionalidade e Estrutura

**Representa√ß√£o Final:**
- **Shape:** `(N_samples, 1024)` onde N_samples = n√∫mero de testes
- **Tipo:** `numpy.ndarray` com dtype `float32`
- **Tamanho em mem√≥ria:** ~4 bytes √ó 1024 √ó N_samples
  - Exemplo: 100K testes = ~400 MB

**Estrutura dos 1024 valores:**
- Cada dimens√£o captura um "aspecto" sem√¢ntico
- Dimens√µes iniciais (0-256): Conceitos gerais (verbos, substantivos)
- Dimens√µes m√©dias (257-768): Contextos e dom√≠nios
- Dimens√µes finais (769-1024): Nuances e detalhes

### 4.2 Propriedades Sem√¢nticas

**Similaridade Cosseno:**

Testes semanticamente similares t√™m embeddings pr√≥ximos:

```python
# Dois testes sobre WiFi
test1 = "Check WiFi connection on device"
test2 = "Verify WiFi connectivity works"
# ‚Üí cosine_similarity(emb1, emb2) ‚âà 0.85

# Teste WiFi vs Bluetooth (diferentes)
test3 = "Test Bluetooth pairing process"
# ‚Üí cosine_similarity(emb1, emb3) ‚âà 0.45
```

**Opera√ß√µes Vetoriais:**

Embeddings suportam √°lgebra vetorial:

```python
# Analogias (exemplo ilustrativo)
embedding("WiFi connection") - embedding("WiFi") + embedding("Bluetooth")
‚âà embedding("Bluetooth connection")
```

### 4.3 Estat√≠sticas T√≠picas

Ap√≥s StandardScaler, os embeddings apresentam:

| M√©trica | Valor T√≠pico | Interpreta√ß√£o |
|---------|--------------|---------------|
| M√©dia | ~0.0 | Centrado na origem |
| Desvio Padr√£o | ~1.0 | Escala normalizada |
| Min | -3.5 a -4.5 | Outliers negativos |
| Max | +3.5 a +4.5 | Outliers positivos |
| Norma L2 | ~30-35 | Magnitude do vetor |

---

## üîó 5. Integra√ß√£o com o Modelo SAINT

### 5.1 Composi√ß√£o Final das Features

Os embeddings sem√¢nticos s√£o combinados com features temporais:

```python
# Embeddings sem√¢nticos: 1024D
semantic_features = embedding_BGE  # Shape: (N, 1024)

# Features temporais: 4D
temporal_features = [
    last_run,        # Bin√°rio: estava no build anterior?
    fail_count,      # log1p(n√∫mero de falhas hist√≥ricas)
    avg_duration,    # log1p(tempo m√©dio de execu√ß√£o)
    run_frequency    # 1/(1 + dias desde √∫ltima execu√ß√£o)
]  # Shape: (N, 4)

# Concatena√ß√£o final
final_features = np.concatenate([semantic_features, temporal_features], axis=1)
# Shape: (N, 1028)
```

**Total:** 1028 dimens√µes = 1024 sem√¢nticas + 4 temporais

### 5.2 Entrada no Modelo SAINT

```python
# Modelo SAINT (config.yaml)
saint:
  num_continuous: 1028   # Todas as features s√£o cont√≠nuas
  num_categorical: 0     # Sem features categ√≥ricas
  embedding_dim: 128     # SAINT embedding interno (diferente do BGE!)
  num_layers: 6
  num_heads: 8
```

**Fluxo de dados:**

```
1024D BGE Embedding ‚îÄ‚îê
                     ‚îú‚îÄ> Concat ‚îÄ> 1028D ‚îÄ> SAINT Embedding Layer ‚îÄ> 128D
4D Temporal Features ‚îÄ‚îò                     (Linear: 1028‚Üí128)
                                                    ‚Üì
                                            Transformer Blocks
                                                    ‚Üì
                                            Classification Head
                                                    ‚Üì
                                            Probability [0,1]
```

**Por que mais uma camada de embedding?**
- SAINT usa sua pr√≥pria representa√ß√£o interna (128D)
- Reduz dimensionalidade para efici√™ncia computacional
- Aprende features n√£o-lineares espec√≠ficas da tarefa
- Permite aten√ß√£o entre diferentes features

---

## üìä 6. Exemplo Completo: Do CSV ao Embedding

### 6.1 Dados Originais (CSV Row)

```csv
Build_ID,TE_Summary,TC_Steps,commit,TE_Test_Result
QPW30.18,"TE - TC - OTA: Download upgrade package","1. Check system update... 2. Select WiFi only...","['IKSWQ-787: support interfaces']","Pass"
```

### 6.2 Ap√≥s M√≥dulo 01 (Parse Commit)

```python
{
    'Build_ID': 'QPW30.18',
    'TE_Summary': 'TE - TC - OTA: Download upgrade package',
    'TC_Steps': '1. Check system update... 2. Select WiFi only...',
    'commit_text': 'IKSWQ-787: support interfaces',  # ‚Üê NOVO
    'TE_Test_Result': 'Pass'
}
```

### 6.3 Ap√≥s M√≥dulo 02 (Build Text Semantic)

```python
{
    ...,
    'text_semantic': '[TE Summary] TE - TC - OTA: Download upgrade package.\n[TC Steps] Step 1: Check system update... Step 2: Select WiFi only...\nIKSWQ-787: support interfaces',  # ‚Üê NOVO
    'label_binary': 0  # ‚Üê NOVO (0=Pass, 1=Fail)
}
```

### 6.4 Ap√≥s M√≥dulo 03 (BGE Embeddings)

```python
{
    ...,
    'embedding': array([
        -0.0234,  0.0156, -0.0421,  0.0389,  # Primeiras dimens√µes
        ...,  # 1016 dimens√µes intermedi√°rias
        -0.0189,  0.0267, -0.0312,  0.0423   # √öltimas dimens√µes
    ], dtype=float32),  # ‚Üê NOVO
    # Shape: (1024,)
}
```

### 6.5 Entrada Final no Modelo (com Temporais)

```python
final_input = np.array([
    # 1024 dimens√µes sem√¢nticas (BGE)
    -0.0234,  0.0156, -0.0421, ..., -0.0312,  0.0423,
    # 4 dimens√µes temporais
    1.0,      # last_run (estava no build anterior)
    0.6931,   # fail_count (log1p(1 falha hist√≥rica))
    2.3026,   # avg_duration (log1p(9 segundos))
    0.8333    # run_frequency (executou h√° 1 dia)
])
# Shape: (1028,)

# Label
label = 0  # Pass
```

---

## ‚öôÔ∏è 7. Configura√ß√µes e Par√¢metros

### 7.1 Arquivo de Configura√ß√£o (config.yaml)

```yaml
# Feature Engineering
features:
  # Semantic features (BGE-large)
  use_semantic: true
  semantic_model: "BAAI/bge-large-en-v1.5"
  semantic_dim: 1024  # Output dimension

  # Temporal features
  use_temporal: true
  temporal_features:
    - "last_run"
    - "fail_count"
    - "avg_duration"
    - "run_frequency"

  # Total
  total_dim: 1028  # 1024 + 4
```

### 7.2 Classe SBERTEmbedder (Principais Par√¢metros)

```python
embedder = SBERTEmbedder(
    model_name='BAAI/bge-large-en-v1.5',
    target_dim=None,        # OBSOLETO (PCA removido)
    batch_size=256,         # Aumentado de 32 para 256
    device='cuda'           # GPU recomendada
)
```

**Nota:** O par√¢metro `target_dim` √© mantido por compatibilidade mas ignorado (PCA foi removido na V5).

### 7.3 Recursos Computacionais

**Tempo de Processamento (estimado):**
- **100 testes:** ~5-10 segundos (GPU) / ~30-60 segundos (CPU)
- **10K testes:** ~8-15 minutos (GPU) / ~45-90 minutos (CPU)
- **100K testes:** ~1.5-3 horas (GPU) / ~8-15 horas (CPU)

**Mem√≥ria Requerida:**
- **Modelo BGE:** ~1.3 GB VRAM (GPU) ou RAM (CPU)
- **Embeddings:** ~400 MB por 100K testes
- **Batch processing:** ~2-4 GB durante encoding

---

## üîç 8. Diferen√ßas da Vers√£o V4 (SBERT) para V5 (BGE)

| Aspecto | V4 (SBERT) | V5 (BGE) | Melhoria |
|---------|-----------|----------|----------|
| **Modelo** | `all-mpnet-base-v2` | `BAAI/bge-large-en-v1.5` | Estado-da-arte |
| **Dimens√µes** | 768 | 1024 | +33% |
| **PCA** | Sim (768‚Üí128) | N√£o (mant√©m 1024) | +700% dimens√µes finais |
| **Total Features** | 772 (768+4) | 1028 (1024+4) | +33% |
| **MTEB Rank** | Top-20 | Top-5 | Melhor retrieval |
| **Treinamento** | Gen√©rico | Otimizado p/ search | Mais robusto |

**Impacto no Desempenho:**
- Melhor captura de nuances sem√¢nticas
- Menor loss de informa√ß√£o (sem PCA)
- Embeddings mais expressivos
- Trade-off: maior custo computacional

---

## üí° 9. Perguntas Frequentes (FAQ)

### Q1: Por que 1024 dimens√µes? N√£o √© muito?

**R:** 1024 dimens√µes s√£o necess√°rias para capturar a complexidade sem√¢ntica de textos t√©cnicos. O modelo BGE foi treinado para produzir embeddings nessa dimensionalidade, e reduzir (com PCA) causaria perda de informa√ß√£o. O SAINT transforma isso em 128D internamente de forma otimizada para a tarefa.

### Q2: O modelo BGE entende portugu√™s?

**R:** N√£o. O `bge-large-en-v1.5` foi treinado apenas em ingl√™s. Se o dataset cont√©m textos em portugu√™s, considere:
- Usar um modelo multil√≠ngue: `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
- Traduzir textos antes de embedding
- Fine-tunar BGE em dados portugueses

### Q3: Como embeddings capturam "significado"?

**R:** Atrav√©s de treinamento em milh√µes de exemplos de pares de textos similares/dissimilares (contrastive learning). O modelo aprende que:
- Sin√¥nimos ‚Üí vetores pr√≥ximos
- Contextos similares ‚Üí vetores similares
- T√≥picos diferentes ‚Üí vetores distantes

### Q4: Posso usar embeddings pr√©-computados?

**R:** Sim! Os embeddings s√£o salvos em `artifacts/embeddings/embeddings_train.npy`. Se o dataset n√£o mudar, voc√™ pode reutiliz√°-los sem reprocessar.

### Q5: Por que StandardScaler e n√£o MinMaxScaler?

**R:** StandardScaler preserva a estrutura sem√¢ntica (transforma√ß√£o linear) e lida melhor com outliers. Embeddings j√° t√™m distribui√ß√£o aproximadamente gaussiana, tornando StandardScaler ideal.

### Q6: O que √© "embedding dimension" do SAINT (128)?

**R:** √â diferente do BGE! O SAINT cria uma nova representa√ß√£o interna de 128D para facilitar aten√ß√£o entre features. Pense em:
- BGE 1024D: representa√ß√£o sem√¢ntica do texto
- SAINT 128D: representa√ß√£o aprendida para classifica√ß√£o

---

## üìö 10. Refer√™ncias e Recursos

### 10.1 Papers Acad√™micos

1. **BGE Model:**
   - BAAI, "C-Pack: Packaged Resources To Advance General Chinese Embedding" (2023)
   - Xiao et al., "BGE: General Embedding Model" (2024)

2. **Sentence Transformers:**
   - Reimers & Gurevych, "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" (2019)

3. **SAINT Model:**
   - Somepalli et al., "SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training" (2021)

### 10.2 C√≥digo Fonte Relevante

| M√≥dulo | Caminho | Fun√ß√£o Principal |
|--------|---------|------------------|
| Parse Commit | `filo_priori/data_processing/01_parse_commit.py` | `parse_commit()` |
| Build Text | `filo_priori/data_processing/02_build_text_semantic.py` | `build_text_semantic()` |
| BGE Embeddings | `filo_priori/data_processing/03_embed_sbert.py` | `SBERTEmbedder.encode()` |
| SAINT Model | `filo_priori/models/saint.py` | `SAINTClassifier` |

### 10.3 Links √öteis

- **BGE Hugging Face:** https://huggingface.co/BAAI/bge-large-en-v1.5
- **Sentence Transformers Docs:** https://www.sbert.net/
- **MTEB Leaderboard:** https://huggingface.co/spaces/mteb/leaderboard

---

## ‚úÖ 11. Checklist de Valida√ß√£o

Para verificar se o sistema de embeddings est√° funcionando corretamente:

- [ ] Modelo BGE carrega sem erros
- [ ] Dimens√£o reportada √© 1024
- [ ] `text_semantic` tem comprimento m√©dio de ~800-1200 chars
- [ ] Embeddings t√™m shape `(N, 1024)`
- [ ] StandardScaler gera m√©dia‚âà0 e std‚âà1
- [ ] N√£o h√° valores NaN ou Inf nos embeddings
- [ ] Scaler √© salvo em `artifacts/embedder/scaler.pkl`
- [ ] Features finais t√™m shape `(N, 1028)`

---

## üìù 12. Conclus√£o

O sistema de embeddings do Filo-Priori V5 √© uma pipeline robusta de 3 etapas que transforma texto bruto de testes de software em representa√ß√µes vetoriais densas de 1024 dimens√µes. O uso do modelo BGE-large-en-v1.5 estado-da-arte, combinado com a remo√ß√£o do PCA e processamento cuidadoso de texto, resulta em embeddings altamente expressivos que capturam nuances sem√¢nticas essenciais para a tarefa de prioriza√ß√£o de testes.

**Principais Takeaways:**
1. **3 colunas ‚Üí 1 embedding:** `TE_Summary` + `TC_Steps` + `commit` ‚Üí 1024D
2. **Sem perda de informa√ß√£o:** PCA removido, todas as 1024 dimens√µes preservadas
3. **Processo determin√≠stico:** Mesmo texto sempre gera mesmo embedding
4. **Escal√°vel:** Processa 100K+ testes em algumas horas (GPU)
5. **Estado-da-arte:** BGE-large √© top-5 em benchmarks MTEB

---

**Documento gerado em:** 2025-10-17
**Vers√£o do Filo-Priori:** V5 (SAINT + BGE)
**Autor:** An√°lise T√©cnica Automatizada
