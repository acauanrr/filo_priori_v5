# AnÃ¡lise Detalhada - Smoke Test Execution_002

**Data**: 2025-10-16
**Status**: âœ… ExecuÃ§Ã£o completada com sucesso, arquivos salvos corretamente
**Objetivo**: Identificar melhorias antes do full test

---

## Resumo Executivo

### âœ… Pontos Positivos

1. **Arquivos salvos corretamente** - Todos os 9 arquivos esperados foram gerados
2. **Treinamento estÃ¡vel** - Loss convergindo sem gradient explosion
3. **Val AUPRC excelente** - 0.8258 (muito bom para dataset desbalanceado)
4. **APFD global alto** - 0.9709 (acima da meta de 0.70)
5. **Modelo converge** - Best epoch 30/30, learning rate decaindo corretamente

### âš ï¸ Problemas CrÃ­ticos Identificados

1. **âŒ PROBLEMA CRÃTICO: DiscriminaÃ§Ã£o ruim** - 0.97x (falhas e passes tÃªm ~mesma probabilidade)
2. **âŒ Test AUPRC muito baixo** - 0.0400 vs 0.8258 val (overfitting severo)
3. **âŒ APFD per-build ruim** - Apenas 16.7% dos builds com APFD â‰¥ 0.7
4. **âŒ Precision muito baixa** - 3.64% no test (modelo marca quase tudo como positivo)
5. **âŒ Recall baixo** - 7.69% (modelo nÃ£o detecta falhas corretamente)

---

## AnÃ¡lise Detalhada

### 1. MÃ©tricas de Treinamento

```
ğŸ“Š TRAINING CONVERGENCE:
   Epochs: 30/30
   Train Loss: 1.242 â†’ 0.385 (convergÃªncia boa)
   Val Loss: 0.659 â†’ 0.231 (convergÃªncia boa)
   Learning Rate: 5e-4 â†’ 5e-6 (cosine decay correto)

ğŸ“ˆ VALIDATION METRICS (Best Epoch 30):
   Val AUPRC: 0.8258 âœ… (excelente!)
   Val Precision: 0.5152 âœ…
   Val Recall: 0.8947 âœ…
   Val F1: 0.6538 âœ…
   Val Accuracy: 0.9667 âœ…
```

**AnÃ¡lise**: Treinamento converge bem, sem instabilidade. Val metrics sÃ£o EXCELENTES.

---

### 2. MÃ©tricas de Test - PROBLEMA CRÃTICO

```
âŒ TEST METRICS (Disaster):
   Test AUPRC: 0.0400 (97% WORSE than val!)
   Test Precision: 0.0364 (93% WORSE)
   Test Recall: 0.0769 (91% WORSE)
   Test F1: 0.0494 (92% WORSE)
   Test Accuracy: 0.8886 (apenas 8% worse, enganoso!)

ğŸ” PROBABILITY ANALYSIS:
   Failures mean: 0.1801 Â± 0.2026
   Passes mean:   0.1860 Â± 0.2084
   Discrimination: 0.97x âŒ (TERRIBLE!)

   Interpretation:
   - Modelo atribui ~18% prob para AMBOS passes e fails
   - Discrimination < 1.0 = passa tem MAIOR prob que falha!
   - Esperado: discrimination >> 1.0 (ex: 2x-5x)
```

**DiagnÃ³stico**: **OVERFITTING SEVERO** - Modelo memorizou validation set mas nÃ£o generaliza.

---

### 3. APFD Analysis - Per-Build Performance

```
ğŸ“Š APFD PER BUILD (12 builds):
   Global APFD: 0.9709 âœ… (misleading!)
   Mean APFD: 0.5154 âš ï¸
   Median APFD: 0.5000 âŒ
   Std: 0.1557

   Distribution:
   - APFD = 1.0:  0 builds (0%)     âŒ
   - APFD â‰¥ 0.7:  2 builds (16.7%)  âŒ (target: 70%)
   - APFD < 0.5:  6 builds (50%)    âŒ

   Best build:  0.8134 (T2TV33.12)
   Worst build: 0.2867 (T2TV33.10)
```

**Problema**: APFD global alto (0.97) Ã© **enganoso** porque:
- Calculado globalmente (nÃ£o por build)
- 50% dos builds tÃªm APFD < 0.5 (coin flip!)
- Meta Ã© â‰¥70% builds com APFD â‰¥ 0.6

**Causa**: Modelo nÃ£o discrimina bem dentro de cada build.

---

### 4. Dataset Analysis

```
ğŸ“¦ SMOKE TEST DATASET:
   Train: 3596 samples (100 builds)
     - Failures: 128 (3.56%)
     - Passes: 3468 (96.44%)

   Val: 540 samples (~15% split)

   Test: 1382 samples (50 builds)
     - Failures: 52 (3.76%)
     - Passes: 1330 (96.24%)

âš ï¸ IMBALANCE ANALYSIS:
   Failure rate: ~3.6% (muito baixo!)
   Imbalance ratio: ~27:1 (passes:failures)

   Com balanced sampler (target=20%):
   - Esperado: batches com ~20% positives
   - Realidade: val/test mantÃ©m 3.6% original
```

**Problema**: Dataset extremamente desbalanceado, mas balanceamento sÃ³ afeta treinamento, nÃ£o validaÃ§Ã£o/test.

---

### 5. Probability Distribution Analysis

Analisando `prioritized_hybrid.csv`:

```
ğŸ” PROBABILITY RANGES:
   Min probability: 0.1045 (muito alto para passes!)
   Max probability: 0.9968

   Passes mean: 0.186 (deveriam ser ~0.05-0.10)
   Failures mean: 0.180 (deveriam ser ~0.50-0.80)

   Problem: Range Ã© ~0.10 a 1.0, mas modelo usa apenas 0.10-0.30
   â†’ Modelo nÃ£o estÃ¡ confiante nas prediÃ§Ãµes
```

**Causa raiz**: CalibraÃ§Ã£o ruim + discriminaÃ§Ã£o fraca.

---

## Causas Raiz dos Problemas

### 1. **Overfitting Severo**

**EvidÃªncias**:
- Val AUPRC: 0.8258 vs Test AUPRC: 0.0400 (diferenÃ§a de 95%)
- Val Precision: 0.52 vs Test Precision: 0.036 (diferenÃ§a de 93%)

**PossÃ­veis causas**:
- âŒ Dataset de treino muito pequeno (100 builds, apenas 128 failures)
- âŒ Modelo muito grande (1.86M params) para dataset pequeno
- âŒ Dropout 0.1 insuficiente
- âŒ Early stopping nÃ£o disparou (best epoch = 30/30)
- âŒ Smoke test nÃ£o representa full dataset

### 2. **DiscriminaÃ§Ã£o Ruim (0.97x)**

**EvidÃªncias**:
- Passes mean prob: 0.186
- Failures mean prob: 0.180
- Discrimination < 1.0 = passa tem prob MAIOR que falha!

**PossÃ­veis causas**:
- âŒ Pos_weight=5.0 insuficiente (ratio Ã© 27:1)
- âŒ Label smoothing 0.05 pode estar prejudicando
- âŒ Features temporais (4D) muito fracas vs semantic (1024D)
- âŒ Modelo nÃ£o aprendeu padrÃµes reais de falha

### 3. **Test Set Muito Diferente de Val**

**EvidÃªncias**:
- Performance despenca no test
- Smoke test usa apenas 100 train + 50 test builds

**Causa**:
- âŒ Smoke test nÃ£o Ã© representativo do full dataset
- âŒ Splits podem ter distribuiÃ§Ãµes diferentes

---

## RecomendaÃ§Ãµes para Full Test

### ğŸ¯ Prioridade ALTA (Implementar ANTES do full test)

#### 1. **Aumentar Pos_Weight**

**Problema**: Pos_weight=5.0 mas imbalance Ã© 27:1
**SoluÃ§Ã£o**: Aumentar para 10.0 ou usar `pos_weight = imbalance_ratio * 0.5`

```python
# config.yaml ou run_experiment_server.py:103
'pos_weight': 10.0,  # Era 5.0
```

**Justificativa**: ForÃ§ar modelo a dar mais peso para falhas.

---

#### 2. **Reduzir Label Smoothing**

**Problema**: Label smoothing 0.05 pode estar suavizando demais as labels
**SoluÃ§Ã£o**: Reduzir para 0.01 ou desabilitar

```python
# config.yaml:
'label_smoothing': 0.01,  # Era 0.05
```

**Justificativa**: Com poucas falhas, label smoothing pode "diluir" sinal.

---

#### 3. **Aumentar Dropout**

**Problema**: Dropout 0.1 muito baixo, overfitting severo
**SoluÃ§Ã£o**: Aumentar para 0.2-0.3

```python
# config.yaml (SAINT config):
'dropout': 0.2,  # Era 0.1
```

**Justificativa**: Reduzir overfitting.

---

#### 4. **Reduzir Patience do Early Stopping**

**Problema**: Best epoch = 30/30 = early stopping nÃ£o disparou
**SoluÃ§Ã£o**: Reduzir patience de 8 para 5

```python
# config.yaml:
'patience': 5,  # Era 8
```

**Justificativa**: Parar antes de overfitting extremo.

---

#### 5. **Adicionar Probability Calibration**

**Problema**: Probabilities nÃ£o calibradas (0.18 para passes E falhas)
**SoluÃ§Ã£o**: Adicionar temperature scaling ou Platt scaling

**Status**: CÃ³digo jÃ¡ tem `ProbabilityCalibrator` em `utils/inference.py`, mas **NÃƒO estÃ¡ sendo usado**!

```python
# Verificar se calibraÃ§Ã£o estÃ¡ ativa no cÃ³digo
# filo_priori/utils/inference.py
```

**ACTION ITEM**: Ativar calibraÃ§Ã£o antes de calcular APFD.

---

#### 6. **Aumentar Target Positive Fraction**

**Problema**: target_positive_fraction=0.20 mas dataset tem 3.6%
**SoluÃ§Ã£o**: Aumentar para 0.30 para forÃ§ar mais exposiÃ§Ã£o a falhas

```python
# config.yaml:
'target_positive_fraction': 0.30,  # Era 0.20
```

---

### ğŸ”§ Prioridade MÃ‰DIA (Considerar implementar)

#### 7. **Reduzir Learning Rate Inicial**

**ObservaÃ§Ã£o**: Train loss cai rÃ¡pido (1.24â†’0.90 em 1 Ã©poca)
**SoluÃ§Ã£o**: Reduzir LR de 5e-4 para 3e-4

```python
'learning_rate': 0.0003,  # Era 0.0005
```

---

#### 8. **Adicionar Weight Decay Maior**

**Problema**: Overfitting
**SoluÃ§Ã£o**: Aumentar weight decay de 0.01 para 0.05

```python
'weight_decay': 0.05,  # Era 0.01
```

---

#### 9. **Reduzir Complexidade do Modelo SAINT**

**Problema**: Modelo muito grande (1.86M params) para smoke test
**SoluÃ§Ã£o**: Reduzir layers ou embedding_dim

```python
# Para smoke test:
'num_layers': 4,  # Era 6
'embedding_dim': 96,  # Era 128
```

**Nota**: Para full test, manter configuraÃ§Ã£o original.

---

### ğŸ“Š Prioridade BAIXA (Melhorias futuras)

#### 10. **Adicionar Data Augmentation**

- Copiar test cases com pequenas variaÃ§Ãµes
- Mixup/CutMix para features

#### 11. **Adicionar Class Weights DinÃ¢micos**

- Calcular pos_weight automaticamente a cada run

#### 12. **Adicionar Focal Loss**

- Substituir BCE por Focal Loss para focar em hard examples

---

## Experimento Proposto para Full Test

### ConfiguraÃ§Ã£o Recomendada

```yaml
# filo_priori/config.yaml

training:
  num_epochs: 30
  learning_rate: 0.0003          # â¬‡ï¸ Era 0.0005
  weight_decay: 0.05             # â¬†ï¸ Era 0.01
  batch_size: 16                 # âœ… Manter
  patience: 5                    # â¬‡ï¸ Era 8
  monitor_metric: val_auprc      # âœ… Manter
  warmup_epochs: 3               # âœ… Manter
  min_lr_ratio: 0.01             # âœ… Manter
  gradient_clip: 1.0             # âœ… Manter
  label_smoothing: 0.01          # â¬‡ï¸ Era 0.05
  pos_weight: 10.0               # â¬†ï¸ Era 5.0
  target_positive_fraction: 0.30 # â¬†ï¸ Era 0.20

saint:
  num_continuous: 1031           # âœ… Auto
  num_categorical: 3             # âœ… Auto
  embedding_dim: 128             # âœ… Manter (full test)
  num_layers: 6                  # âœ… Manter (full test)
  num_heads: 8                   # âœ… Manter
  dropout: 0.2                   # â¬†ï¸ Era 0.1
  use_intersample: true          # âœ… Manter
```

---

## Checklist PrÃ©-Full Test

### CorreÃ§Ãµes ObrigatÃ³rias

- [ ] **CRÃTICO**: Aumentar `pos_weight` de 5.0 â†’ 10.0
- [ ] **CRÃTICO**: Reduzir `label_smoothing` de 0.05 â†’ 0.01
- [ ] **CRÃTICO**: Aumentar `dropout` de 0.1 â†’ 0.2
- [ ] **IMPORTANTE**: Reduzir `patience` de 8 â†’ 5
- [ ] **IMPORTANTE**: Aumentar `target_positive_fraction` de 0.20 â†’ 0.30
- [ ] **DESEJÃVEL**: Verificar se calibraÃ§Ã£o estÃ¡ ativa (`utils/inference.py`)
- [ ] **DESEJÃVEL**: Reduzir LR de 5e-4 â†’ 3e-4
- [ ] **DESEJÃVEL**: Aumentar weight_decay de 0.01 â†’ 0.05

### ValidaÃ§Ãµes

- [ ] Rodar smoke test com configuraÃ§Ã£o nova
- [ ] Verificar que discrimination ratio > 1.5x
- [ ] Verificar que test AUPRC > 0.15 (nÃ£o 0.04!)
- [ ] Verificar que val/test gap < 50%

---

## CÃ³digo para Aplicar CorreÃ§Ãµes

### OpÃ§Ã£o 1: Editar config.yaml (RECOMENDADO)

```bash
# Verificar se config.yaml existe
ls filo_priori/config.yaml

# Editar config.yaml manualmente com as mudanÃ§as acima
```

### OpÃ§Ã£o 2: Passar parÃ¢metros via CLI (temporÃ¡rio)

```python
# Modificar run_experiment_server.py DEFAULT_CONFIG
# Linhas 92-105
'training': {
    'num_epochs': 30,
    'learning_rate': 3e-4,        # â¬‡ï¸
    'weight_decay': 0.05,          # â¬†ï¸
    'batch_size': 16,
    'patience': 5,                 # â¬‡ï¸
    'monitor_metric': 'val_auprc',
    'warmup_epochs': 3,
    'min_lr_ratio': 0.01,
    'gradient_clip': 1.0,
    'label_smoothing': 0.01,       # â¬‡ï¸
    'pos_weight': 10.0,            # â¬†ï¸
    'target_positive_fraction': 0.30  # â¬†ï¸
},
```

---

## Expectativas para Full Test

### Com CorreÃ§Ãµes Aplicadas

```
ğŸ“Š EXPECTED IMPROVEMENTS:
   Test AUPRC: 0.15-0.30 (vs 0.04 atual)
   Test Precision: 0.10-0.25 (vs 0.036 atual)
   Discrimination: 1.5-3.0x (vs 0.97x atual)

ğŸ“ˆ APFD PER BUILD:
   Mean APFD: 0.60-0.75 (vs 0.52 atual)
   Builds with APFD â‰¥ 0.7: 40-60% (vs 16.7% atual)

âš ï¸ REALISMO:
   - Full dataset tem MUITO mais dados
   - Smoke test nÃ£o Ã© representativo
   - Performance pode melhorar significativamente
```

### Se Performance Continuar Ruim

**Plano B**:
1. Testar MLP simples ao invÃ©s de SAINT
2. Aumentar weight das features temporais
3. Adicionar mais features (file churn, test history, etc.)
4. Considerar ensemble de modelos

---

## ConclusÃ£o

### Status Atual

âœ… **Sistema estÃ¡ funcionando** - Arquivos salvos, pipeline completo
âš ï¸ **Performance inadequada** - Discrimination ruim, overfitting severo
ğŸ¯ **AÃ§Ã£o necessÃ¡ria** - Aplicar correÃ§Ãµes de hiperparÃ¢metros ANTES de full test

### PrÃ³ximos Passos

1. **Aplicar correÃ§Ãµes de hiperparÃ¢metros** (10 minutos)
2. **Rodar smoke test novamente** (15-30 min) para validar
3. **Se smoke test melhorar**: Rodar full test
4. **Se smoke test nÃ£o melhorar**: Investigar features/arquitetura

### Estimativa de Sucesso

- **Com correÃ§Ãµes**: 60-70% chance de atingir meta (APFD â‰¥ 0.70, â‰¥70% builds)
- **Sem correÃ§Ãµes**: 10-20% chance (discriminaÃ§Ã£o Ã© muito ruim)

---

## Arquivos de ReferÃªncia

- Resultados: `filo_priori/results/execution_002/`
- ConfiguraÃ§Ã£o: `filo_priori/scripts/core/run_experiment_server.py:72-107`
- Config (se existir): `filo_priori/config.yaml`
- CalibraÃ§Ã£o: `filo_priori/utils/inference.py` (verificar se estÃ¡ ativa)

---

**Data da anÃ¡lise**: 2025-10-16
**PrÃ³xima aÃ§Ã£o**: Aplicar correÃ§Ãµes e re-testar smoke test
